{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codesearchnet_2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPd3O1Xl7CA"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Ar53sBl7CB"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVvzNA7l7CB",
        "outputId": "f971cc24-5cd6-40c4-d4f9-9cefd1bd6385"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K     |████████████████████████████████| 235kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 23.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 41.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 42.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1CKuTdVl7CE",
        "outputId": "24b07e60-ceed-47cb-f8c6-45518b0b89a7"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up GCS access...\n",
            "Running on TPU: grpc://10.88.248.130:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3EkFkJ21G6p"
      },
      "source": [
        "# print(mesh_tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A969X_jAr2qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8adab681-6cea-418b-ae9e-4a4d1db6a683"
      },
      "source": [
        "print(t5.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPI1TmAeh0_"
      },
      "source": [
        "# import gin\n",
        "# import subprocess\n",
        "# gin.parse_config_file(\n",
        "#         'gs://t5-data/pretrained_models/base/operative_config.gin'\n",
        "#     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtIvOasl7CI"
      },
      "source": [
        "## Register codesearchnet Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt5jaNDnLMF4"
      },
      "source": [
        "### java\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCs8MoxiLMF4",
        "outputId": "05ec4485-195d-4a82-be5f-ebf4733403f1"
      },
      "source": [
        "def dumping_dataset_java(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/java/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"java: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_java(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public ImageSource apply ( ImageSource input ) { final int [ ] [ ] pixelMatrix = new int [ 3 ] [ 3 ] ; int w = input . getWidth ( ) ; int h = input . getHeight ( ) ; int [ ] [ ] output = new int [ h ] [ w ] ; for ( int j = 1 ; j < h - 1 ; j ++ ) { for ( int i = 1 ; i < w - 1 ; i ++ ) { pixelMatrix [ 0 ] [ 0 ] = input . getR ( i - 1 , j - 1 ) ; pixelMatrix [ 0 ] [ 1 ] = input . getRGB ( i - 1 , j ) ; pixelMatrix [ 0 ] [ 2 ] = input . getRGB ( i - 1 , j + 1 ) ; pixelMatrix [ 1 ] [ 0 ] = input . getRGB ( i , j - 1 ) ; pixelMatrix [ 1 ] [ 2 ] = input . getRGB ( i , j + 1 ) ; pixelMatrix [ 2 ] [ 0 ] = input . getRGB ( i + 1 , j - 1 ) ; pixelMatrix [ 2 ] [ 1 ] = input . getRGB ( i + 1 , j ) ; pixelMatrix [ 2 ] [ 2 ] = input . getRGB ( i + 1 , j + 1 ) ; int edge = ( int ) convolution ( pixelMatrix ) ; int rgb = ( edge << 16 | edge << 8 | edge ) ; output [ j ] [ i ] = rgb ; } } MatrixSource source = new MatrixSource ( output ) ; return source ; }', 'target': b'Expects a height mat as input'}\n",
            "{'input': b'public < L extends Listener > void popEvent ( Event < ? , L > expected ) { synchronized ( this . stack ) { final Event < ? , ? > actual = this . stack . pop ( ) ; if ( actual != expected ) { throw new IllegalStateException ( String . format ( \"Unbalanced pop: expected \\'%s\\' but encountered \\'%s\\'\" , expected . getListenerClass ( ) , actual ) ) ; } } }', 'target': b'Pops the top event off the current event stack . This action has to be performed immediately after the event has been dispatched to all listeners .'}\n",
            "{'input': b'protected void modify ( Transaction t ) { try { this . lock . writeLock ( ) . lock ( ) ; t . perform ( ) ; } finally { this . lock . writeLock ( ) . unlock ( ) ; } }', 'target': b'Executes the given transaction within the context of a write lock .'}\n",
            "{'input': b'protected < E > E read ( Supplier < E > sup ) { try { this . lock . readLock ( ) . lock ( ) ; return sup . get ( ) ; } finally { this . lock . readLock ( ) . unlock ( ) ; } }', 'target': b'Executes the given supplier within the context of a read lock .'}\n",
            "{'input': b'protected void setOffsetAndLength ( long offset , int length ) throws IOException { this . offset = offset ; this . length = length ; this . position = 0 ; if ( subStream . position ( ) != offset ) { subStream . seek ( offset ) ; } }', 'target': b'This should be called from a subclass constructor if offset or length are unknown at a time when SubIIMInputStream constructor is called . This method shouldn t be called more than once .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57ip-NtLMF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba703bd-af36-4ef5-e903-12c584e5eb61"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_java,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fcfec96d890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfw_jz9uLLy2"
      },
      "source": [
        "### php\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geh3lKLGLLy2",
        "outputId": "cef5de48-6c04-4162-ebf6-379a2b8585e4"
      },
      "source": [
        "def dumping_dataset_php(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/php/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"php: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_php(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b\"public function onChannelPreDelete ( ResourceControllerEvent $ event ) : void { $ channel = $ event -> getSubject ( ) ; if ( ! $ channel instanceof ChannelInterface ) { throw new UnexpectedTypeException ( $ channel , ChannelInterface :: class ) ; } $ results = $ this -> channelRepository -> findBy ( [ 'enabled' => true ] ) ; if ( ! $ results || ( count ( $ results ) === 1 && current ( $ results ) === $ channel ) ) { $ event -> stop ( 'sylius.channel.delete_error' ) ; } }\", 'target': b'Prevent channel deletion if no more channels enabled .'}\n",
            "{'input': b'public function getTaxTotal ( ) : int { $ taxTotal = 0 ; foreach ( $ this -> getAdjustments ( AdjustmentInterface :: TAX_ADJUSTMENT ) as $ taxAdjustment ) { $ taxTotal += $ taxAdjustment -> getAmount ( ) ; } foreach ( $ this -> units as $ unit ) { $ taxTotal += $ unit -> getTaxTotal ( ) ; } return $ taxTotal ; }', 'target': b'Returns sum of neutral and non neutral tax adjustments on order item and total tax of units .'}\n",
            "{'input': b'private function isLastEnabledEntity ( $ result , $ entity ) : bool { return ! $ result || 0 === count ( $ result ) || ( 1 === count ( $ result ) && $ entity === ( $ result instanceof \\\\ Iterator ? $ result -> current ( ) : current ( $ result ) ) ) ; }', 'target': b'If no entity matched the query criteria or a single entity matched which is the same as the entity being validated the entity is the last enabled entity available .'}\n",
            "{'input': b'protected function recalculateTotal ( ) : void { $ this -> total = $ this -> itemsTotal + $ this -> adjustmentsTotal ; if ( $ this -> total < 0 ) { $ this -> total = 0 ; } }', 'target': b'Items total + Adjustments total .'}\n",
            "{'input': b\"public function loginAction ( Request $ request ) : Response { $ authenticationUtils = $ this -> get ( 'security.authentication_utils' ) ; $ error = $ authenticationUtils -> getLastAuthenticationError ( ) ; $ lastUsername = $ authenticationUtils -> getLastUsername ( ) ; $ options = $ request -> attributes -> get ( '_sylius' ) ; $ template = $ options [ 'template' ] ?? null ; Assert :: notNull ( $ template , 'Template is not configured.' ) ; $ formType = $ options [ 'form' ] ?? UserLoginType :: class ; $ form = $ this -> get ( 'form.factory' ) -> createNamed ( '' , $ formType ) ; return $ this -> render ( $ template , [ 'form' => $ form -> createView ( ) , 'last_username' => $ lastUsername , 'error' => $ error , ] ) ; }\", 'target': b'Login form action .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39Rl4-QLLy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c1ee94-dbd7-4924-c938-63ae05cab0cc"
      },
      "source": [
        "t5.data.TaskRegistry.remove('php')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"php\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_php,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fd0afd54590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ujnPGvLLLkI"
      },
      "source": [
        "### js\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkRMCkgXLLkI",
        "outputId": "39840263-1622-45a2-fc15-492fb2c9aa75"
      },
      "source": [
        "def dumping_dataset_js(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/javascript/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"javascript: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_js(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'function ( state , action ) { return _ . defaults ( { isValidating : action . isValidating , lastAction : IS_VALIDATING } , state ) }', 'target': b'Update is validating result'}\n",
            "{'input': b'function addWidgetForFilter ( view , filter , editModeHint ) { var gridster = view . _widgetsGridster ; var row = filter . row || 1 ; var col = filter . col || 1 ; var sizeX = filter . size_x || 3 ; var sizeY = filter . size_y || 3 ; var el = gridster . add_widget ( \\'<div class=\"widgetOuterFrame\"></div>\\' , sizeX , sizeY , col , row ) ; var frameView = new WidgetFrameView ( { model : filter } ) ; view . renderSubview ( frameView , el [ 0 ] ) ; frameView . renderContent ( ) ; frameView . gridsterHook = el [ 0 ] ; $ ( el [ 0 ] ) . data ( \\'spotWidgetFrameView\\' , frameView ) ; var chartView = frameView . widget ; chartView . model . updateConfiguration ( ) ; if ( chartView . model . isConfigured ) { if ( ! filter . isInitialized ) { filter . initDataFilter ( ) ; } if ( ! chartView . isInitialized ) { chartView . initChart ( ) ; } chartView . update ( ) ; frameView . editMode = editModeHint ; } else { frameView . editMode = true ; } filter . on ( \\'newData\\' , function ( ) { chartView . update ( ) ; } ) ; }', 'target': b'Add a widget to the analyze page for the given filter'}\n",
            "{'input': b\"function inRange ( value , min , max ) { const int = parseInt ( value , 10 ) return ( ` ${ int } ` === ` ${ value . replace ( / ^0 / , '' ) } ` && int >= min && int <= max ) }\", 'target': b'Determine if value is within a numeric range'}\n",
            "{'input': b\"function markdown ( options ) { return new Remarkable ( extend ( { breaks : false , html : true , langPrefix : 'lang-' , linkify : true , typographer : false , xhtmlOut : false } , options ) ) ; }\", 'target': b'Shared settings for remarkable'}\n",
            "{'input': b\"function partitionValueToIndex ( partition , value ) { var group ; if ( ! partition ) { return 0 ; } group = partition . groups . get ( value , 'value' ) ; if ( group ) { return group . groupIndex ; } else { return - 1 ; } }\", 'target': b'Get the index in chartjs datastructures from the group value with proper fallbacks'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy-_Utt1LLkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99706888-fb23-4bc9-eb28-23620a5d8dd7"
      },
      "source": [
        "t5.data.TaskRegistry.remove('js')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"js\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_js,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fcfb6aa26d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZl55kWLK3y"
      },
      "source": [
        "### ruby\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcjUh8YCLK35",
        "outputId": "7eb53f21-4512-4723-d091-7f64c554ead8"
      },
      "source": [
        "def dumping_dataset_ruby(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/ruby/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"ruby: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_ruby(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'def render_body ( context , options ) if options . key? ( :partial ) [ render_partial ( context , options ) ] else StreamingTemplateRenderer . new ( @lookup_context ) . render ( context , options ) end end', 'target': b'Render but returns a valid Rack body . If fibers are defined we return a streaming body that renders the template piece by piece .'}\n",
            "{'input': b'def attribute_missing ( match , * args , & block ) __send__ ( match . target , match . attr_name , * args , & block ) end', 'target': b'+ attribute_missing + is like + method_missing + but for attributes . When + method_missing + is called we check to see if there is a matching attribute method . If so we tell + attribute_missing + to dispatch the attribute . This method can be overloaded to customize the behavior .'}\n",
            "{'input': b'def matched_attribute_method ( method_name ) matches = self . class . send ( :attribute_method_matchers_matching , method_name ) matches . detect { | match | attribute_method? ( match . attr_name ) } end', 'target': b'Returns a struct representing the matching attribute method . The struct s attributes are prefix base and suffix .'}\n",
            "{'input': b'def demodulize ( path ) path = path . to_s if i = path . rindex ( \"::\" ) path [ ( i + 2 ) .. - 1 ] else path end end', 'target': b'Removes the module part from the expression in the string .'}\n",
            "{'input': b'def const_regexp ( camel_cased_word ) parts = camel_cased_word . split ( \"::\" ) return Regexp . escape ( camel_cased_word ) if parts . blank? last = parts . pop parts . reverse . inject ( last ) do | acc , part | part . empty? ? acc : \"#{part}(::#{acc})?\" end end', 'target': b'Mounts a regular expression returned as a string to ease interpolation that will match part by part the given constant .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWoF_w7ILK36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2432f61-1b91-40d0-d98c-5826bc102f55"
      },
      "source": [
        "t5.data.TaskRegistry.remove('ruby')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"ruby\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_ruby,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fcfb6ad1210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JgSsuel7EC"
      },
      "source": [
        "### go\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJt-8jsUl7EC",
        "outputId": "fa49fb5c-3e4f-46ee-c9e1-aead0b665eb7"
      },
      "source": [
        "def dumping_dataset_go(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/go/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"go: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_go(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'func getAllDepTypes ( ) [ ] string { depTypes := make ( [ ] string , 0 , len ( cmds ) ) \\\\n for depType := range cmds { depTypes = append ( depTypes , depType ) \\\\n } \\\\n sort . Strings ( depTypes ) \\\\n return depTypes \\\\n }', 'target': b'getAllDepTypes returns a sorted list of names of all dep type commands .'}\n",
            "{'input': b'func getIoProgressReader ( label string , res * http . Response ) io . Reader { prefix := \"Downloading \" + label \\\\n fmtBytesSize := 18 \\\\n barSize := int64 ( 80 - len ( prefix ) - fmtBytesSize ) \\\\n bar := ioprogress . DrawTextFormatBarForW ( barSize , os . Stderr ) \\\\n fmtfunc := func ( progress , total int64 ) string { if total == - 1 { return fmt . Sprintf ( \"%s: %v of an unknown total size\" , prefix , ioprogress . ByteUnitStr ( progress ) , ) \\\\n } \\\\n return fmt . Sprintf ( \"%s: %s %s\" , prefix , bar ( progress , total ) , ioprogress . DrawTextFormatBytes ( progress , total ) , ) \\\\n } \\\\n return & ioprogress . Reader { Reader : res . Body , Size : res . ContentLength , DrawFunc : ioprogress . DrawTerminalf ( os . Stderr , fmtfunc ) , DrawInterval : time . Second , } \\\\n }', 'target': b'getIoProgressReader returns a reader that wraps the HTTP response body so it prints a pretty progress bar when reading data from it .'}\n",
            "{'input': b'func ( f * removeOnClose ) Close ( ) error { if f == nil || f . File == nil { return nil \\\\n } \\\\n name := f . File . Name ( ) \\\\n if err := f . File . Close ( ) ; err != nil { return err \\\\n } \\\\n if err := os . Remove ( name ) ; err != nil && ! os . IsNotExist ( err ) { return err \\\\n } \\\\n return nil \\\\n }', 'target': b'Close closes the file and then removes it from disk . No error is returned if the file did not exist at the point of removal .'}\n",
            "{'input': b'func getTmpROC ( s * imagestore . Store , path string ) ( * removeOnClose , error ) { h := sha512 . New ( ) \\\\n h . Write ( [ ] byte ( path ) ) \\\\n pathHash := s . HashToKey ( h ) \\\\n tmp , err := s . TmpNamedFile ( pathHash ) \\\\n if err != nil { return nil , errwrap . Wrap ( errors . New ( \"error setting up temporary file\" ) , err ) \\\\n } \\\\n _ , err = lock . TryExclusiveLock ( tmp . Name ( ) , lock . RegFile ) \\\\n if err != nil { if err != lock . ErrLocked { return nil , errwrap . Wrap ( errors . New ( \"failed to lock temporary file\" ) , err ) \\\\n } \\\\n log . Printf ( \"another rkt instance is downloading this file, waiting...\" ) \\\\n _ , err = lock . ExclusiveLock ( tmp . Name ( ) , lock . RegFile ) \\\\n if err != nil { return nil , errwrap . Wrap ( errors . New ( \"failed to lock temporary file\" ) , err ) \\\\n } \\\\n } \\\\n return & removeOnClose { File : tmp } , nil \\\\n }', 'target': b'getTmpROC returns a removeOnClose instance wrapping a temporary file provided by the passed store . The actual file name is based on a hash of the passed path .'}\n",
            "{'input': b'func getStage1Entrypoint ( cdir string , entrypoint string ) ( string , error ) { b , err := ioutil . ReadFile ( common . Stage1ManifestPath ( cdir ) ) \\\\n if err != nil { return \"\" , errwrap . Wrap ( errors . New ( \"error reading pod manifest\" ) , err ) \\\\n } \\\\n s1m := schema . ImageManifest { } \\\\n if err := json . Unmarshal ( b , & s1m ) ; err != nil { return \"\" , errwrap . Wrap ( errors . New ( \"error unmarshaling stage1 manifest\" ) , err ) \\\\n } \\\\n if ep , ok := s1m . Annotations . Get ( entrypoint ) ; ok { return ep , nil \\\\n } \\\\n return \"\" , fmt . Errorf ( \"entrypoint %q not found\" , entrypoint ) \\\\n }', 'target': b'getStage1Entrypoint retrieves the named entrypoint from the stage1 manifest for a given pod'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhK5bXycl7EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bd419c-49c8-48e7-88e0-c034e840d916"
      },
      "source": [
        "t5.data.TaskRegistry.remove('go')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"go\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_go,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fcfb6b0f610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFoA73ouMBM1"
      },
      "source": [
        "### python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnIhn9K_MBM-",
        "outputId": "a1d49264-82f0-4a7f-c06a-c10534e6f8ad"
      },
      "source": [
        "def dumping_dataset_python(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://t5_training/t5-data/code_data/codesearchnet/python/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"python: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_python(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]', 'target': b'Return either the full or truncated version of a QIIME - formatted taxonomy string .'}\n",
            "{'input': b'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )', 'target': b'Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .'}\n",
            "{'input': b'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle', 'target': b'Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .'}\n",
            "{'input': b'def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table', 'target': b'Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .'}\n",
            "{'input': b'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )', 'target': b'Parses the unifrac results file into a dictionary'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spun7-5XMBM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ae335b-1717-42cd-c152-cfd3349fc8d2"
      },
      "source": [
        "t5.data.TaskRegistry.remove('python')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"python\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_python,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7fcfb6a7c210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceK0-uXzl7EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eafa6f3-a01b-4414-fbb4-04ee37294254"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"all_mix\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_mix\",\n",
        "    [\n",
        "     'go',\n",
        "     'ruby',\n",
        "     'js',\n",
        "     'php',\n",
        "     'java',\n",
        "     'python',\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7fcfb6a8d250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwvZLQy4wv0I"
      },
      "source": [
        "# !gsutil -m rm -r {MODEL_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e74kZFIZJoYJ",
        "outputId": "aeeca95a-1e82-46d1-9205-2e015c49241a"
      },
      "source": [
        "!gsutil -m cp gs://t5_training/models/code/codesummarization_uni_v1/base/* gs://t5_training/models/code/codesummarization_uni_v1_1/base/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/checkpoint...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1196000.data-00001-of-00002...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/graph.pbtxt...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1203000.index...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1196000.data-00000-of-00002...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1196000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/events.out.tfevents.1617638570.98969c0f035c...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \r/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1196000.meta...\n",
            "/ [0/57 files][    0.0 B/  6.0 GiB]   0% Done                                   \rCopying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1203000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1203000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1203000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1209900.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1209900.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1209900.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1209900.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1216500.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1216500.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1216500.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1216500.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1224000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1224000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1224000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1229700.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1224000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1229700.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1229700.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1229700.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230800.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230800.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230800.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1230800.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231900.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231900.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231900.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1231900.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1232000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1232000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1232000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1232000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1233000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1233000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1233000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1233000.meta...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1234000.data-00000-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1234000.data-00001-of-00002...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1234000.index...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/operative_config.gin...\n",
            "Copying gs://t5_training/models/code/codesummarization_uni_v1/base/model.ckpt-1234000.meta...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSzYqw_l7EP"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "BASE_PRETRAINED_DIR = \"gs://t5_training/models/code/code_uni_v1\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v1\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pubmed_v2\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/export_models/bio/pmc_v4_1200k\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "# MODEL_DIR = \"gs://t5_training/models/bio/re_v2\"\n",
        "MODEL_DIR = \"gs://t5_training/models/code/codesummarization_uni_v1_1\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBkygCUDl7EQ",
        "outputId": "5295f35b-b42f-4bbd-8955-fb4899ace0ad"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/code_uni_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/code_uni_v1/base/operative_config.gin\n",
            "INFO:root:Skipping import of unknown module `t5.data.sentencepiece_vocabulary` (skip_unknown=True).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://t5_training/models/code/codesummarization_uni_v1_1/base', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.88.248.130:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.88.248.130:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.88.248.130:8470', '_evaluation_master': 'grpc://10.88.248.130:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7fcfb517bb50>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.88.248.130:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.88.248.130:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 2718840973995841455)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8193441938583471030)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -6047518998276255690)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6079379393641019390)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4366199960851821407)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5409543674024665268)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4492315436190166906)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -1633536714866283128)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1363873021653202986)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 4952773247367005723)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 3764492779010763832)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[4, 2] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[2] physical_shape=[1, 1, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1)]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=4, model=2] LayoutRules{('vocab', 'model'), ('ensemble', 'ensemble'), ('batch', 'batch'), ('d_ff', 'model'), ('heads', 'model'), ('experts', 'batch')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7fcfb3830490>\n",
            "INFO:tensorflow:serialize_num_microbatches: tokens_per_microbatch_per_replica=8192 batch_dim=Dimension(name='batch', size=128) sequence_length={'inputs': 512, 'targets': 512} batch_per_replica=32 num_microbatches=2\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_006/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_007/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_008/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_009/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_010/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/k                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/o                size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/q                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_001/EncDecAttention/v                size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable decoder/block_011/layer_002/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_006/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_007/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_008/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_009/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_010/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/k                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/o                  size 589824       slice_size 294912       Shape[heads=768, d_model=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/q                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_000/SelfAttention/v                  size 589824       slice_size 294912       Shape[d_model=768, heads=768]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wi/kernel         size 2359296      slice_size 1179648      Shape[d_model=768, d_ff=3072]                               \n",
            "INFO:tensorflow:Variable encoder/block_011/layer_001/DenseReluDense/wo/kernel         size 2359296      slice_size 1179648      Shape[d_ff=3072, d_model=768]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 24674304     slice_size 12337152     Shape[vocab=32128, d_model=768]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 768          slice_size 384          Shape[stacked=2, heads=12, buckets=32]                      \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 47616        slice_size 47616        Shape[stacked=62, d_model=768]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_006/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_006/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_007/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_007/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_008/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_008/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_009/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_009/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_010/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_010/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_011/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_011/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_006/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_007/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_008/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_009/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_010/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_011/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 195     Total size: 222903552        Total slice_size: 111475584      \n",
            "INFO:tensorflow:All Variables                  count: 203     Total size: 223390336        Total slice_size: 111816896      \n",
            "INFO:tensorflow:Counters:\n",
            "allreduce: 7.44e+09\n",
            " allreduce/[0]: 1.19e+09\n",
            "  allreduce/[0]/einsum_op: 8.92e+08\n",
            "  allreduce/[0]/reduce_op: 3.02e+08\n",
            " allreduce/[1]: 6.24e+09\n",
            "  allreduce/[1]/einsum_op: 6.24e+09\n",
            "  allreduce/[1]/reduce_op: 1.45e+06\n",
            "einsum: 2.97e+13\n",
            "einsum_unique: 2.96e+13\n",
            "output: 1.07e+10\n",
            " output/AddOperation: 2.73e+06\n",
            " output/Constant: 8\n",
            " output/EinsumOperation: 3.57e+09\n",
            " output/ImportOperation: 3.15e+06\n",
            " output/MinMaxOperation: 4.11e+03\n",
            " output/ReduceOperation: 2.35e+06\n",
            " output/ReshapeOperation: 7.86e+05\n",
            " output/ScalarAddOperation: 8.92e+08\n",
            " output/ScalarMultiplyOperation: 7.82e+06\n",
            " output/SlicewiseOperation: 4.46e+09\n",
            " output/StackOperation: 2.99e+06\n",
            " output/StackedVariable: 2.99e+06\n",
            " output/UnstackOperation: 2.99e+06\n",
            " output/Variable: 8.92e+08\n",
            " output/WhileLoopOperation: 8.92e+08\n",
            "output_unique: 2.68e+09\n",
            " output_unique/AddOperation: 4.87e+05\n",
            " output_unique/Constant: 1\n",
            " output_unique/EinsumOperation: 8.92e+08\n",
            " output_unique/ImportOperation: 3.94e+05\n",
            " output_unique/MinMaxOperation: 514\n",
            " output_unique/ReduceOperation: 4.39e+05\n",
            " output_unique/ReshapeOperation: 3.93e+05\n",
            " output_unique/ScalarAddOperation: 2.23e+08\n",
            " output_unique/ScalarMultiplyOperation: 1.41e+06\n",
            " output_unique/SlicewiseOperation: 1.12e+09\n",
            " output_unique/StackOperation: 5.03e+05\n",
            " output_unique/StackedVariable: 5.03e+05\n",
            " output_unique/UnstackOperation: 5.03e+05\n",
            " output_unique/Variable: 2.23e+08\n",
            " output_unique/WhileLoopOperation: 2.23e+08\n",
            "variables: 2.23e+08\n",
            " variables/trainable: 2.23e+08\n",
            " variables/untrainable: 4.87e+05\n",
            "INFO:tensorflow:Initializing variables from gs://t5_training/models/code/code_uni_v1/base/model.ckpt-1200000:\n",
            "INFO:tensorflow:Variables in gs://t5_training/models/code/code_uni_v1/base/model.ckpt-1200000 but not in graph:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Variables in graph but not in gs://t5_training/models/code/code_uni_v1/base/model.ckpt-1200000:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Starting the session.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt-1234000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1076: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1234000...\n",
            "INFO:tensorflow:Saving checkpoints for 1234000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1234000...\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1234000...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1234000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1234000...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 60)\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1234100\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (1, 15)\n",
            "INFO:tensorflow:Outfeed finished for iteration (1, 75)\n",
            "INFO:tensorflow:loss = 0.026855469, step = 1234200 (103.913 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.962317\n",
            "INFO:tensorflow:examples/sec: 123.177\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (2, 35)\n",
            "INFO:tensorflow:Outfeed finished for iteration (2, 95)\n",
            "INFO:tensorflow:loss = 0.025390625, step = 1234300 (101.374 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98647\n",
            "INFO:tensorflow:examples/sec: 126.268\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (3, 53)\n",
            "INFO:tensorflow:loss = 0.029296875, step = 1234400 (103.432 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966818\n",
            "INFO:tensorflow:examples/sec: 123.753\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (4, 13)\n",
            "INFO:tensorflow:Outfeed finished for iteration (4, 73)\n",
            "INFO:tensorflow:loss = 0.036132812, step = 1234500 (101.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986632\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (5, 30)\n",
            "INFO:tensorflow:Outfeed finished for iteration (5, 90)\n",
            "INFO:tensorflow:loss = 0.025146484, step = 1234600 (104.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.958812\n",
            "INFO:tensorflow:examples/sec: 122.728\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (6, 50)\n",
            "INFO:tensorflow:loss = 0.032714844, step = 1234700 (103.368 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967409\n",
            "INFO:tensorflow:examples/sec: 123.828\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (7, 8)\n",
            "INFO:tensorflow:Outfeed finished for iteration (7, 68)\n",
            "INFO:tensorflow:loss = 0.025390625, step = 1234800 (101.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986604\n",
            "INFO:tensorflow:examples/sec: 126.285\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (8, 28)\n",
            "INFO:tensorflow:Outfeed finished for iteration (8, 88)\n",
            "INFO:tensorflow:loss = 0.028686523, step = 1234900 (101.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986614\n",
            "INFO:tensorflow:examples/sec: 126.287\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (9, 45)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1235000...\n",
            "INFO:tensorflow:Saving checkpoints for 1235000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:970: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1235000...\n",
            "INFO:tensorflow:loss = 0.033203125, step = 1235000 (116.434 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.858849\n",
            "INFO:tensorflow:examples/sec: 109.933\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (10, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (10, 60)\n",
            "INFO:tensorflow:loss = 0.029907227, step = 1235100 (101.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986568\n",
            "INFO:tensorflow:examples/sec: 126.281\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1235100...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1235100 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1235100...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (11, 4)\n",
            "INFO:tensorflow:Outfeed finished for iteration (11, 64)\n",
            "INFO:tensorflow:loss = 0.030761719, step = 1235200 (116.852 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.855787\n",
            "INFO:tensorflow:examples/sec: 109.541\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (12, 24)\n",
            "INFO:tensorflow:Outfeed finished for iteration (12, 84)\n",
            "INFO:tensorflow:loss = 0.028564453, step = 1235300 (103.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968085\n",
            "INFO:tensorflow:examples/sec: 123.915\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (13, 42)\n",
            "INFO:tensorflow:loss = 0.026367188, step = 1235400 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98658\n",
            "INFO:tensorflow:examples/sec: 126.282\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (14, 2)\n",
            "INFO:tensorflow:Outfeed finished for iteration (14, 62)\n",
            "INFO:tensorflow:loss = 0.027709961, step = 1235500 (101.356 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986616\n",
            "INFO:tensorflow:examples/sec: 126.287\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (15, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (15, 80)\n",
            "INFO:tensorflow:loss = 0.033447266, step = 1235600 (103.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967566\n",
            "INFO:tensorflow:examples/sec: 123.848\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (16, 40)\n",
            "INFO:tensorflow:loss = 0.03515625, step = 1235700 (101.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986674\n",
            "INFO:tensorflow:examples/sec: 126.294\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (17, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (17, 60)\n",
            "INFO:tensorflow:loss = 0.032714844, step = 1235800 (103.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966681\n",
            "INFO:tensorflow:examples/sec: 123.735\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (18, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (18, 80)\n",
            "INFO:tensorflow:loss = 0.03100586, step = 1235900 (103.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968166\n",
            "INFO:tensorflow:examples/sec: 123.925\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (19, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (19, 98)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1236000...\n",
            "INFO:tensorflow:Saving checkpoints for 1236000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1236000...\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1236000 (114.689 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.871929\n",
            "INFO:tensorflow:examples/sec: 111.607\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (20, 45)\n",
            "INFO:tensorflow:loss = 0.030517578, step = 1236100 (101.367 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986534\n",
            "INFO:tensorflow:examples/sec: 126.276\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (21, 3)\n",
            "INFO:tensorflow:Outfeed finished for iteration (21, 63)\n",
            "INFO:tensorflow:loss = 0.030517578, step = 1236200 (103.239 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968632\n",
            "INFO:tensorflow:examples/sec: 123.985\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1236200...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1236200 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1236200...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (22, 9)\n",
            "INFO:tensorflow:Outfeed finished for iteration (22, 69)\n",
            "INFO:tensorflow:loss = 0.030151367, step = 1236300 (115.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.867582\n",
            "INFO:tensorflow:examples/sec: 111.051\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (23, 27)\n",
            "INFO:tensorflow:Outfeed finished for iteration (23, 87)\n",
            "INFO:tensorflow:loss = 0.030029297, step = 1236400 (103.416 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966975\n",
            "INFO:tensorflow:examples/sec: 123.773\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (24, 47)\n",
            "INFO:tensorflow:loss = 0.032958984, step = 1236500 (103.334 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967737\n",
            "INFO:tensorflow:examples/sec: 123.87\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (25, 5)\n",
            "INFO:tensorflow:Outfeed finished for iteration (25, 65)\n",
            "INFO:tensorflow:loss = 0.028808594, step = 1236600 (101.361 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986557\n",
            "INFO:tensorflow:examples/sec: 126.279\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (26, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (26, 85)\n",
            "INFO:tensorflow:loss = 0.030639648, step = 1236700 (101.354 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986648\n",
            "INFO:tensorflow:examples/sec: 126.291\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (27, 43)\n",
            "INFO:tensorflow:loss = 0.033203125, step = 1236800 (103.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968113\n",
            "INFO:tensorflow:examples/sec: 123.919\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (28, 3)\n",
            "INFO:tensorflow:Outfeed finished for iteration (28, 63)\n",
            "INFO:tensorflow:loss = 0.028320312, step = 1236900 (101.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986634\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (29, 21)\n",
            "INFO:tensorflow:Outfeed finished for iteration (29, 81)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1237000...\n",
            "INFO:tensorflow:Saving checkpoints for 1237000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1237000...\n",
            "INFO:tensorflow:loss = 0.026245117, step = 1237000 (118.902 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.841029\n",
            "INFO:tensorflow:examples/sec: 107.652\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (30, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (30, 85)\n",
            "INFO:tensorflow:loss = 0.026367188, step = 1237100 (103.605 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96517\n",
            "INFO:tensorflow:examples/sec: 123.542\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (31, 42)\n",
            "INFO:tensorflow:loss = 0.027954102, step = 1237200 (101.372 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986466\n",
            "INFO:tensorflow:examples/sec: 126.268\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (32, 2)\n",
            "INFO:tensorflow:Outfeed finished for iteration (32, 62)\n",
            "INFO:tensorflow:loss = 0.032714844, step = 1237300 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986606\n",
            "INFO:tensorflow:examples/sec: 126.286\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1237300...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1237300 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1237300...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (33, 7)\n",
            "INFO:tensorflow:Outfeed finished for iteration (33, 67)\n",
            "INFO:tensorflow:loss = 0.025390625, step = 1237400 (116.733 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.856631\n",
            "INFO:tensorflow:examples/sec: 109.649\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (34, 27)\n",
            "INFO:tensorflow:Outfeed finished for iteration (34, 87)\n",
            "INFO:tensorflow:loss = 0.028808594, step = 1237500 (101.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986622\n",
            "INFO:tensorflow:examples/sec: 126.288\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (35, 45)\n",
            "INFO:tensorflow:loss = 0.026123047, step = 1237600 (103.268 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968358\n",
            "INFO:tensorflow:examples/sec: 123.95\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (36, 5)\n",
            "INFO:tensorflow:Outfeed finished for iteration (36, 65)\n",
            "INFO:tensorflow:loss = 0.029785156, step = 1237700 (103.368 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967433\n",
            "INFO:tensorflow:examples/sec: 123.831\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (37, 23)\n",
            "INFO:tensorflow:Outfeed finished for iteration (37, 83)\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1237800 (101.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986574\n",
            "INFO:tensorflow:examples/sec: 126.281\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (38, 43)\n",
            "INFO:tensorflow:loss = 0.027954102, step = 1237900 (101.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986559\n",
            "INFO:tensorflow:examples/sec: 126.28\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (39, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (39, 60)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1238000...\n",
            "INFO:tensorflow:Saving checkpoints for 1238000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1238000...\n",
            "INFO:tensorflow:loss = 0.026611328, step = 1238000 (117.686 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.849704\n",
            "INFO:tensorflow:examples/sec: 108.762\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (40, 7)\n",
            "INFO:tensorflow:Outfeed finished for iteration (40, 67)\n",
            "INFO:tensorflow:loss = 0.029907227, step = 1238100 (101.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986676\n",
            "INFO:tensorflow:examples/sec: 126.295\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (41, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (41, 85)\n",
            "INFO:tensorflow:loss = 0.030151367, step = 1238200 (103.230 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968722\n",
            "INFO:tensorflow:examples/sec: 123.996\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (42, 45)\n",
            "INFO:tensorflow:loss = 0.029418945, step = 1238300 (103.404 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967064\n",
            "INFO:tensorflow:examples/sec: 123.784\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (43, 3)\n",
            "INFO:tensorflow:Outfeed finished for iteration (43, 63)\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1238400 (101.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986612\n",
            "INFO:tensorflow:examples/sec: 126.286\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1238400...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1238400 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1238400...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (44, 7)\n",
            "INFO:tensorflow:Outfeed finished for iteration (44, 67)\n",
            "INFO:tensorflow:loss = 0.029174805, step = 1238500 (116.867 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.855687\n",
            "INFO:tensorflow:examples/sec: 109.528\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (45, 24)\n",
            "INFO:tensorflow:Outfeed finished for iteration (45, 84)\n",
            "INFO:tensorflow:loss = 0.026123047, step = 1238600 (103.603 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96522\n",
            "INFO:tensorflow:examples/sec: 123.548\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (46, 44)\n",
            "INFO:tensorflow:loss = 0.029052734, step = 1238700 (101.357 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98665\n",
            "INFO:tensorflow:examples/sec: 126.291\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (47, 2)\n",
            "INFO:tensorflow:Outfeed finished for iteration (47, 62)\n",
            "INFO:tensorflow:loss = 0.028564453, step = 1238800 (103.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968145\n",
            "INFO:tensorflow:examples/sec: 123.923\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (48, 22)\n",
            "INFO:tensorflow:Outfeed finished for iteration (48, 82)\n",
            "INFO:tensorflow:loss = 0.033691406, step = 1238900 (103.307 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967993\n",
            "INFO:tensorflow:examples/sec: 123.903\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (49, 40)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1239000...\n",
            "INFO:tensorflow:Saving checkpoints for 1239000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1239000...\n",
            "INFO:tensorflow:loss = 0.0234375, step = 1239000 (114.891 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.870404\n",
            "INFO:tensorflow:examples/sec: 111.412\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (50, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (50, 60)\n",
            "INFO:tensorflow:loss = 0.030273438, step = 1239100 (101.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986628\n",
            "INFO:tensorflow:examples/sec: 126.288\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (51, 18)\n",
            "INFO:tensorflow:Outfeed finished for iteration (51, 78)\n",
            "INFO:tensorflow:loss = 0.02709961, step = 1239200 (103.375 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967332\n",
            "INFO:tensorflow:examples/sec: 123.818\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (52, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (52, 98)\n",
            "INFO:tensorflow:loss = 0.030029297, step = 1239300 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98662\n",
            "INFO:tensorflow:examples/sec: 126.287\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (53, 56)\n",
            "INFO:tensorflow:loss = 0.029296875, step = 1239400 (103.361 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967478\n",
            "INFO:tensorflow:examples/sec: 123.837\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (54, 16)\n",
            "INFO:tensorflow:Outfeed finished for iteration (54, 76)\n",
            "INFO:tensorflow:loss = 0.028442383, step = 1239500 (103.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968564\n",
            "INFO:tensorflow:examples/sec: 123.976\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1239500...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1239500 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1239500...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (55, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (55, 80)\n",
            "INFO:tensorflow:loss = 0.025146484, step = 1239600 (115.832 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.86334\n",
            "INFO:tensorflow:examples/sec: 110.508\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (56, 40)\n",
            "INFO:tensorflow:loss = 0.029052734, step = 1239700 (101.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986602\n",
            "INFO:tensorflow:examples/sec: 126.285\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (57, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (57, 60)\n",
            "INFO:tensorflow:loss = 0.030029297, step = 1239800 (103.306 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967965\n",
            "INFO:tensorflow:examples/sec: 123.9\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (58, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (58, 80)\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1239900 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986606\n",
            "INFO:tensorflow:examples/sec: 126.286\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (59, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (59, 98)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1240000...\n",
            "INFO:tensorflow:Saving checkpoints for 1240000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1240000...\n",
            "INFO:tensorflow:loss = 0.02758789, step = 1240000 (117.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.853237\n",
            "INFO:tensorflow:examples/sec: 109.214\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (60, 44)\n",
            "INFO:tensorflow:loss = 0.030517578, step = 1240100 (103.414 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966987\n",
            "INFO:tensorflow:examples/sec: 123.774\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (61, 2)\n",
            "INFO:tensorflow:Outfeed finished for iteration (61, 62)\n",
            "INFO:tensorflow:loss = 0.031982422, step = 1240200 (101.354 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986628\n",
            "INFO:tensorflow:examples/sec: 126.288\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (62, 22)\n",
            "INFO:tensorflow:Outfeed finished for iteration (62, 82)\n",
            "INFO:tensorflow:loss = 0.032226562, step = 1240300 (101.357 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986602\n",
            "INFO:tensorflow:examples/sec: 126.285\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (63, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (63, 98)\n",
            "INFO:tensorflow:loss = 0.028198242, step = 1240400 (104.831 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.953912\n",
            "INFO:tensorflow:examples/sec: 122.101\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (64, 58)\n",
            "INFO:tensorflow:loss = 0.03125, step = 1240500 (101.354 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986654\n",
            "INFO:tensorflow:examples/sec: 126.292\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (65, 16)\n",
            "INFO:tensorflow:Outfeed finished for iteration (65, 76)\n",
            "INFO:tensorflow:loss = 0.030273438, step = 1240600 (103.372 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967368\n",
            "INFO:tensorflow:examples/sec: 123.823\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1240600...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1240600 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1240600...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (66, 23)\n",
            "INFO:tensorflow:Outfeed finished for iteration (66, 83)\n",
            "INFO:tensorflow:loss = 0.033935547, step = 1240700 (116.343 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.859511\n",
            "INFO:tensorflow:examples/sec: 110.017\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (67, 41)\n",
            "INFO:tensorflow:loss = 0.027954102, step = 1240800 (101.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986549\n",
            "INFO:tensorflow:examples/sec: 126.278\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (68, 1)\n",
            "INFO:tensorflow:Outfeed finished for iteration (68, 61)\n",
            "INFO:tensorflow:loss = 0.0234375, step = 1240900 (101.358 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986615\n",
            "INFO:tensorflow:examples/sec: 126.287\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (69, 19)\n",
            "INFO:tensorflow:Outfeed finished for iteration (69, 79)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1241000...\n",
            "INFO:tensorflow:Saving checkpoints for 1241000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1241000...\n",
            "INFO:tensorflow:loss = 0.025146484, step = 1241000 (117.292 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.852579\n",
            "INFO:tensorflow:examples/sec: 109.13\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (70, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (70, 85)\n",
            "INFO:tensorflow:loss = 0.025390625, step = 1241100 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986576\n",
            "INFO:tensorflow:examples/sec: 126.282\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (71, 43)\n",
            "INFO:tensorflow:loss = 0.03515625, step = 1241200 (103.327 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967823\n",
            "INFO:tensorflow:examples/sec: 123.881\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (72, 3)\n",
            "INFO:tensorflow:Outfeed finished for iteration (72, 63)\n",
            "INFO:tensorflow:loss = 0.029541016, step = 1241300 (104.485 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.957026\n",
            "INFO:tensorflow:examples/sec: 122.499\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (73, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (73, 80)\n",
            "INFO:tensorflow:loss = 0.028808594, step = 1241400 (101.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98661\n",
            "INFO:tensorflow:examples/sec: 126.286\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (74, 40)\n",
            "INFO:tensorflow:loss = 0.026000977, step = 1241500 (101.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986586\n",
            "INFO:tensorflow:examples/sec: 126.283\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (75, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (75, 60)\n",
            "INFO:tensorflow:loss = 0.02368164, step = 1241600 (103.382 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967255\n",
            "INFO:tensorflow:examples/sec: 123.809\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (76, 20)\n",
            "INFO:tensorflow:Outfeed finished for iteration (76, 80)\n",
            "INFO:tensorflow:loss = 0.028076172, step = 1241700 (101.357 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986651\n",
            "INFO:tensorflow:examples/sec: 126.291\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1241700...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1241700 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1241700...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (77, 23)\n",
            "INFO:tensorflow:Outfeed finished for iteration (77, 83)\n",
            "INFO:tensorflow:loss = 0.025146484, step = 1241800 (117.825 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.848707\n",
            "INFO:tensorflow:examples/sec: 108.634\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (78, 43)\n",
            "INFO:tensorflow:loss = 0.033447266, step = 1241900 (103.305 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967955\n",
            "INFO:tensorflow:examples/sec: 123.898\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (79, 1)\n",
            "INFO:tensorflow:Outfeed finished for iteration (79, 61)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1242000...\n",
            "INFO:tensorflow:Saving checkpoints for 1242000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1242000...\n",
            "INFO:tensorflow:loss = 0.026611328, step = 1242000 (115.702 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.86433\n",
            "INFO:tensorflow:examples/sec: 110.634\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (80, 7)\n",
            "INFO:tensorflow:Outfeed finished for iteration (80, 67)\n",
            "INFO:tensorflow:loss = 0.028808594, step = 1242100 (101.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986629\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (81, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (81, 85)\n",
            "INFO:tensorflow:loss = 0.026977539, step = 1242200 (103.409 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.967079\n",
            "INFO:tensorflow:examples/sec: 123.786\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (82, 45)\n",
            "INFO:tensorflow:loss = 0.030761719, step = 1242300 (101.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986645\n",
            "INFO:tensorflow:examples/sec: 126.291\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (83, 3)\n",
            "INFO:tensorflow:Outfeed finished for iteration (83, 63)\n",
            "INFO:tensorflow:loss = 0.030761719, step = 1242400 (103.378 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96727\n",
            "INFO:tensorflow:examples/sec: 123.811\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (84, 23)\n",
            "INFO:tensorflow:Outfeed finished for iteration (84, 83)\n",
            "INFO:tensorflow:loss = 0.025146484, step = 1242500 (103.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.965846\n",
            "INFO:tensorflow:examples/sec: 123.628\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (85, 41)\n",
            "INFO:tensorflow:loss = 0.02709961, step = 1242600 (101.365 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986544\n",
            "INFO:tensorflow:examples/sec: 126.278\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (86, 1)\n",
            "INFO:tensorflow:Outfeed finished for iteration (86, 61)\n",
            "INFO:tensorflow:loss = 0.031982422, step = 1242700 (101.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986671\n",
            "INFO:tensorflow:examples/sec: 126.294\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (87, 19)\n",
            "INFO:tensorflow:Outfeed finished for iteration (87, 79)\n",
            "INFO:tensorflow:loss = 0.029541016, step = 1242800 (103.385 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96727\n",
            "INFO:tensorflow:examples/sec: 123.811\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1242800...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1242800 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1242800...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (88, 25)\n",
            "INFO:tensorflow:Outfeed finished for iteration (88, 85)\n",
            "INFO:tensorflow:loss = 0.030761719, step = 1242900 (114.793 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.871127\n",
            "INFO:tensorflow:examples/sec: 111.504\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (89, 43)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1243000...\n",
            "INFO:tensorflow:Saving checkpoints for 1243000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1243000...\n",
            "INFO:tensorflow:loss = 0.023925781, step = 1243000 (117.418 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.851653\n",
            "INFO:tensorflow:examples/sec: 109.012\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (90, 0)\n",
            "INFO:tensorflow:Outfeed finished for iteration (90, 60)\n",
            "INFO:tensorflow:loss = 0.024902344, step = 1243100 (103.421 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966929\n",
            "INFO:tensorflow:examples/sec: 123.767\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (91, 18)\n",
            "INFO:tensorflow:Outfeed finished for iteration (91, 78)\n",
            "INFO:tensorflow:loss = 0.026489258, step = 1243200 (101.366 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986532\n",
            "INFO:tensorflow:examples/sec: 126.276\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (92, 38)\n",
            "INFO:tensorflow:Outfeed finished for iteration (92, 98)\n",
            "INFO:tensorflow:loss = 0.030273438, step = 1243300 (101.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.98663\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (93, 56)\n",
            "INFO:tensorflow:loss = 0.02758789, step = 1243400 (103.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966163\n",
            "INFO:tensorflow:examples/sec: 123.669\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (94, 16)\n",
            "INFO:tensorflow:Outfeed finished for iteration (94, 76)\n",
            "INFO:tensorflow:loss = 0.030029297, step = 1243500 (101.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986629\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (95, 34)\n",
            "INFO:tensorflow:Outfeed finished for iteration (95, 94)\n",
            "INFO:tensorflow:loss = 0.028442383, step = 1243600 (103.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968226\n",
            "INFO:tensorflow:examples/sec: 123.933\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (96, 54)\n",
            "INFO:tensorflow:loss = 0.029296875, step = 1243700 (103.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.968378\n",
            "INFO:tensorflow:examples/sec: 123.952\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (97, 12)\n",
            "INFO:tensorflow:Outfeed finished for iteration (97, 72)\n",
            "INFO:tensorflow:loss = 0.023925781, step = 1243800 (101.360 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986594\n",
            "INFO:tensorflow:examples/sec: 126.284\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (98, 32)\n",
            "INFO:tensorflow:Outfeed finished for iteration (98, 92)\n",
            "INFO:tensorflow:loss = 0.025878906, step = 1243900 (101.357 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986586\n",
            "INFO:tensorflow:examples/sec: 126.283\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1243900...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1243900 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1243900...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (99, 36)\n",
            "INFO:tensorflow:Outfeed finished for iteration (99, 96)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1244000...\n",
            "INFO:tensorflow:Saving checkpoints for 1244000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1244000...\n",
            "INFO:tensorflow:loss = 0.028320312, step = 1244000 (132.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.757039\n",
            "INFO:tensorflow:examples/sec: 96.901\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (100, 41)\n",
            "INFO:tensorflow:loss = 0.026367188, step = 1244100 (101.361 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986633\n",
            "INFO:tensorflow:examples/sec: 126.289\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (101, 1)\n",
            "INFO:tensorflow:Outfeed finished for iteration (101, 61)\n",
            "INFO:tensorflow:loss = 0.027832031, step = 1244200 (101.366 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986489\n",
            "INFO:tensorflow:examples/sec: 126.271\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (102, 19)\n",
            "INFO:tensorflow:Outfeed finished for iteration (102, 79)\n",
            "INFO:tensorflow:loss = 0.030761719, step = 1244300 (103.290 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.96818\n",
            "INFO:tensorflow:examples/sec: 123.927\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (103, 39)\n",
            "INFO:tensorflow:Outfeed finished for iteration (103, 99)\n",
            "INFO:tensorflow:loss = 0.024169922, step = 1244400 (101.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986626\n",
            "INFO:tensorflow:examples/sec: 126.288\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (104, 57)\n",
            "INFO:tensorflow:loss = 0.036132812, step = 1244500 (103.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.966696\n",
            "INFO:tensorflow:examples/sec: 123.737\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (105, 17)\n",
            "INFO:tensorflow:Outfeed finished for iteration (105, 77)\n",
            "INFO:tensorflow:loss = 0.029785156, step = 1244600 (103.668 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.964627\n",
            "INFO:tensorflow:examples/sec: 123.472\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (106, 34)\n",
            "INFO:tensorflow:Outfeed finished for iteration (106, 94)\n",
            "INFO:tensorflow:loss = 0.025878906, step = 1244700 (101.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986577\n",
            "INFO:tensorflow:examples/sec: 126.282\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (107, 54)\n",
            "INFO:tensorflow:loss = 0.02734375, step = 1244800 (101.359 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.986584\n",
            "INFO:tensorflow:examples/sec: 126.283\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (108, 11)\n",
            "INFO:tensorflow:Outfeed finished for iteration (108, 71)\n",
            "INFO:tensorflow:loss = 0.02709961, step = 1244900 (104.415 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.957731\n",
            "INFO:tensorflow:examples/sec: 122.59\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (109, 31)\n",
            "INFO:tensorflow:Outfeed finished for iteration (109, 91)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1245000...\n",
            "INFO:tensorflow:Saving checkpoints for 1245000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1245000...\n",
            "INFO:tensorflow:loss = 0.02709961, step = 1245000 (117.599 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.850345\n",
            "INFO:tensorflow:examples/sec: 108.844\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1245000...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 1245000 into gs://t5_training/models/code/codesummarization_uni_v1_1/base/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1245000...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Done with the session.\n",
            "INFO:tensorflow:Loss for final step: 0.02709961.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW"
      },
      "source": [
        "tasks = [\n",
        "         ['codesearchnet', 'python'],\n",
        "         ['codesearchnet', 'java'],\n",
        "         ['codesearchnet', 'javascript'],\n",
        "         ['codesearchnet', 'go'],\n",
        "         ['codesearchnet', 'php'],\n",
        "         ['codesearchnet', 'ruby'],\n",
        "         ]\n",
        "output_dir = \"codesummarization_uni_v1\"\n",
        "test_file = 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6VR3fJS7y2i",
        "outputId": "64674a53-6082-46d3-c891-9140873a2af6"
      },
      "source": [
        "for task in tasks:\n",
        "  lang = task[1]\n",
        "  !mkdir {lang}\n",
        "  !gsutil cp gs://t5_training/t5-data/code_data/{task[0]}/{lang}/{test_file}.tsv {lang}/\n",
        "  with open(f'{lang}/{test_file}.tsv', 'r') as file:\n",
        "    with open(f'{lang}/predict_input.tsv', 'w') as predict_input:\n",
        "      with open(f'{lang}/actual_output.tsv', 'w') as actual_output:\n",
        "        for line in file:\n",
        "          line = line.strip().split('\\t')\n",
        "          input = line[0].strip()\n",
        "          actual = line[1].strip()\n",
        "\n",
        "          predict_input.write(f'{lang}: {input}\\n')\n",
        "          actual_output.write(f'{actual}\\n')\n",
        "# for task in tasks:\n",
        "#   lang = task[1]\n",
        "#   !gsutil cp {lang}/actual_output.tsv gs://t5_training/t5-data/code_data/{task[0]}/{lang}/\n",
        "#   !gsutil cp {lang}/predict_input.tsv gs://t5_training/t5-data/code_data/{task[0]}/{lang}/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘python’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/python/test.tsv...\n",
            "/ [1 files][  7.9 MiB/  7.9 MiB]                                                \n",
            "Operation completed over 1 objects/7.9 MiB.                                      \n",
            "mkdir: cannot create directory ‘java’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/java/test.tsv...\n",
            "/ [1 files][  5.8 MiB/  5.8 MiB]                                                \n",
            "Operation completed over 1 objects/5.8 MiB.                                      \n",
            "mkdir: cannot create directory ‘javascript’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/javascript/test.tsv...\n",
            "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n",
            "mkdir: cannot create directory ‘go’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/go/test.tsv...\n",
            "/ [1 files][  3.9 MiB/  3.9 MiB]                                                \n",
            "Operation completed over 1 objects/3.9 MiB.                                      \n",
            "mkdir: cannot create directory ‘php’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/php/test.tsv...\n",
            "/ [1 files][  7.0 MiB/  7.0 MiB]                                                \n",
            "Operation completed over 1 objects/7.0 MiB.                                      \n",
            "mkdir: cannot create directory ‘ruby’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/ruby/test.tsv...\n",
            "/ [1 files][540.6 KiB/540.6 KiB]                                                \n",
            "Operation completed over 1 objects/540.6 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzDXwIlI16q",
        "outputId": "688862c9-6356-46c9-eb9d-db79b7137ca0"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "# question_1 = \"Emerin is a nuclear membrane protein which is missing or defective in Emery-Dreifuss muscular dystrophy (EDMD). It is one member of a family of lamina-associated proteins which includes LAP1, LAP2 and lamin B receptor (LBR). A panel of 16 monoclonal antibodies (mAbs) has been mapped to six specific sites throughout the emerin molecule using phage-displayed peptide libraries and has been used to localize emerin in human and rabbit heart. Several mAbs against different emerin epitopes did not recognize intercalated discs in the heart, though they recognized cardiomyocyte nuclei strongly, both at the rim and in intranuclear spots or channels. A polyclonal rabbit antiserum against emerin did recognize both nuclear membrane and intercalated discs but, after affinity purification against a pure-emerin band on a western blot, it stained only the nuclear membrane. These results would not be expected if immunostaining at intercalated discs were due to a product of the emerin gene and, therefore, cast some doubt upon the hypothesis that cardiac defects in EDMD are caused by absence of emerin from intercalated discs. Although emerin was abundant in the membranes of cardiomyocyte nuclei, it was absent from many non-myocyte cells in the heart. This distribution of emerin was similar to that of lamin A, a candidate gene for an autosomal form of EDMD. In contrast, lamin B1 was absent from cardiomyocyte nuclei, showing that lamin B1 is not essential for localization of emerin to the nuclear lamina. Lamin B1 is also almost completely absent from skeletal muscle nuclei. In EDMD, the additional absence of lamin B1 from heart and skeletal muscle nuclei which already lack emerin may offer an alternative explanation of why these tissues are particularly affected..\" \n",
        "# question_2 = \"Molecular analysis of the APC gene in 205 families: extended genotype-phenotype correlations in FAP and evidence for the role of APC amino acid changes in colorectal cancer predisposition.\" \n",
        "# question_3 = \"Who are the 4 members of The Beatles?\" \n",
        "# question_4 = \"How many teeth do humans have?\"\n",
        "\n",
        "# questions = [question_2]\n",
        "\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  lang = t[1]\n",
        "  input_file = f'{lang}/predict_input.tsv'\n",
        "  output_file = f'{lang}/predict_output.tsv'\n",
        "\n",
        "\n",
        "  # Write out the supplied questions to text files.\n",
        "  # predict_inputs_path = os.path.join('gs://t5_training/t5-data/code_data', dir, input_file)\n",
        "  # predict_outputs_path = os.path.join('gs://t5_training/t5-data/code_data', dir, output_dir , MODEL_SIZE, output_file)\n",
        "\n",
        "  predict_inputs_path = input_file\n",
        "  predict_outputs_path = output_file\n",
        "\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "  print(predict_inputs_path)\n",
        "  print(predict_outputs_path)\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        checkpoint_steps=-1,\n",
        "        # Select the most probable output token at each step.\n",
        "        # vocabulary=t5.data.SentencePieceVocabulary(vocab)\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + lang)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "  # with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
        "  #   for q, a in zip(questions, f):\n",
        "  #     if q:\n",
        "  #       print(\"Q: \" + q)\n",
        "  #       print(\"A: \" + a)\n",
        "  #       print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "python/predict_input.tsv\n",
            "python/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : python\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "java/predict_input.tsv\n",
            "java/predict_output.tsv\n",
            "Predicted task : java\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "javascript/predict_input.tsv\n",
            "javascript/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : javascript\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "go/predict_input.tsv\n",
            "go/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : go\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "php/predict_input.tsv\n",
            "php/predict_output.tsv\n",
            "Predicted task : php\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "ruby/predict_input.tsv\n",
            "ruby/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_uni_v1_1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : ruby\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['codesearchnet', 'python'],\n",
        "         ['codesearchnet', 'java'],\n",
        "         ['codesearchnet', 'javascript'],\n",
        "         ['codesearchnet', 'go'],\n",
        "         ['codesearchnet', 'php'],\n",
        "         ['codesearchnet', 'ruby'],\n",
        "         ]\n",
        "output_dir = \"codesummarization_uni_v1\"\n",
        "test_file = 'test'\n",
        "checkpoint = '1234000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7odWGzG7p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b7c70c-f090-45b8-d429-2e275dc50a2f"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/evaluator.py\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/predictions.txt\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/reference.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-23 00:07:44--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/evaluator.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6916 (6.8K) [text/plain]\n",
            "Saving to: ‘evaluator.py.1’\n",
            "\n",
            "\revaluator.py.1        0%[                    ]       0  --.-KB/s               \revaluator.py.1      100%[===================>]   6.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 00:07:44 (49.2 MB/s) - ‘evaluator.py.1’ saved [6916/6916]\n",
            "\n",
            "--2021-04-23 00:07:45--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/predictions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196 [text/plain]\n",
            "Saving to: ‘predictions.txt.1’\n",
            "\n",
            "predictions.txt.1   100%[===================>]     196  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 00:07:45 (7.72 MB/s) - ‘predictions.txt.1’ saved [196/196]\n",
            "\n",
            "--2021-04-23 00:07:45--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/reference.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381 [text/plain]\n",
            "Saving to: ‘reference.txt.1’\n",
            "\n",
            "reference.txt.1     100%[===================>]     381  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 00:07:45 (5.68 MB/s) - ‘reference.txt.1’ saved [381/381]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfCRXczMMaeF",
        "outputId": "5483167b-86b1-4bc1-e568-6f5cd1d0001b"
      },
      "source": [
        "!python evaluator.py reference.txt < predictions.txt\n",
        "!mkdir output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 5\n",
            "9.554726113590661\n",
            "mkdir: cannot create directory ‘output’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gURjp726NqO1",
        "outputId": "1d9f97cd-f825-40cb-cecc-1d0d3f1b475f"
      },
      "source": [
        "for task in tasks:\n",
        "  lang = task[1]\n",
        "  with open(f'{lang}/predict_output.tsv-{checkpoint}') as predict_output:\n",
        "    with open(f'{lang}/actual_output.tsv') as actual_output:\n",
        "      with open(f'output/{lang}_reference.txt', 'w') as reference:\n",
        "        with open(f'output/{lang}_predictions.txt', 'w') as predictions:\n",
        "          for idx, (line1, line2) in enumerate(zip(predict_output, actual_output)):\n",
        "            line1 = line1.replace('⁇', '')\n",
        "            reference.write(f'{idx}\\t{line2}')\n",
        "            predictions.write(f'{idx}\\t{line1}')\n",
        "          print(f'language: {lang}')\n",
        "          !python evaluator.py output/{lang}_reference.txt < output/{lang}_predictions.txt\n",
        "          print('\\n')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language: python\n",
            "Total: 14779\n",
            "19.731685546321724\n",
            "\n",
            "\n",
            "language: java\n",
            "Total: 10861\n",
            "19.079527564567012\n",
            "\n",
            "\n",
            "language: javascript\n",
            "Total: 3200\n",
            "14.965278143124621\n",
            "\n",
            "\n",
            "language: go\n",
            "Total: 7989\n",
            "18.899657295993357\n",
            "\n",
            "\n",
            "language: php\n",
            "Total: 13820\n",
            "24.603038462225744\n",
            "\n",
            "\n",
            "language: ruby\n",
            "Total: 1199\n",
            "14.037486232881045\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pzdjOUFD_iC",
        "outputId": "5082c490-cf01-409d-deb4-6a18995e4897"
      },
      "source": [
        "!zip -r output.zip output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: output/ (stored 0%)\n",
            "updating: output/ruby_predictions.txt (deflated 65%)\n",
            "updating: output/python_reference.txt (deflated 68%)\n",
            "updating: output/php_predictions.txt (deflated 70%)\n",
            "updating: output/java_predictions.txt (deflated 72%)\n",
            "updating: output/php_reference.txt (deflated 69%)\n",
            "updating: output/go_reference.txt (deflated 70%)\n",
            "updating: output/python_predictions.txt (deflated 71%)\n",
            "updating: output/javascript_predictions.txt (deflated 68%)\n",
            "updating: output/go_predictions.txt (deflated 73%)\n",
            "updating: output/java_reference.txt (deflated 69%)\n",
            "updating: output/javascript_reference.txt (deflated 66%)\n",
            "updating: output/ruby_reference.txt (deflated 65%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SLpoaaiUs81"
      },
      "source": [
        "language: python\n",
        "Total: 14882\n",
        "71.75255231835018\n",
        "\n",
        "\n",
        "language: java\n",
        "Total: 10899\n",
        "16.06278592746895\n",
        "\n",
        "\n",
        "language: javascript\n",
        "Total: 3254\n",
        "7.793938400469148\n",
        "\n",
        "\n",
        "language: go\n",
        "Total: 8009\n",
        "22.548576422978876\n",
        "\n",
        "\n",
        "language: php\n",
        "Total: 14001\n",
        "21.239563344111858\n",
        "\n",
        "\n",
        "language: ruby\n",
        "Total: 1204\n",
        "7.628737805048416\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuvjLJZDHDNQ",
        "outputId": "a31bc5e8-1f99-4b2b-97cf-856444b52256"
      },
      "source": [
        "checkpoint = 1245000\n",
        "total_f1 = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "anchor_pred_labels = []\n",
        "anchor_actual_labels = []\n",
        "for task in tasks:\n",
        "    d = task[0]\n",
        "    t = task[1]\n",
        "    \n",
        "    pred_file = os.path.join('/content/', t +'_predict_output.txt-%s'%checkpoint)\n",
        "    actual_file = os.path.join('/content/', t + '_actual_output.txt')\n",
        "    \n",
        "    # pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "    # actual_file = 'test_raw.txt'\n",
        "    pred_labels = convert_RE_labels(pred_file)\n",
        "    actual_labels = convert_RE_labels(actual_file)\n",
        "#     print(pred_labels)\n",
        "#     print(actual_labels)\n",
        "#     pred_labels = np.zeros(len(actual_labels)).tolist()\n",
        "\n",
        "    \n",
        "    # f1score = f1_score(actual_labels, pred_labels, average='micro')\n",
        "    # recallscore = recall_score(actual_labels, pred_labels, average='micro')\n",
        "    # precisionscore = precision_score(actual_labels, pred_labels, average='micro')\n",
        "\n",
        "    # total_f1 += f1score\n",
        "    # total_recall += recallscore\n",
        "    # total_precision += precisionscore\n",
        "    # accuracy = accuracy_score(actual_labels, pred_labels)\n",
        "    # print(t , f1score, recallscore, precisionscore)\n",
        "    # break\n",
        "    \n",
        "#     f1score = f1_score(tmp_actual, tmp_pred)\n",
        "#     recallscore = recall_score(tmp_actual, tmp_pred)\n",
        "#     precisionscore = precision_score(tmp_actual, tmp_pred)\n",
        "    \n",
        "#     print(\"%s\\t Precision: %2f \\t Recall-score: %2f \\t F1-score: %2f \" % (t, precisionscore, recallscore, f1score))\n",
        "#     print(\"Accuracy score: %2f\" % accuracy_score(actual_labels, pred_labels))\n",
        "#     print(t)|\n",
        "    # print(\"Report:\", classification_report(actual_labels, pred_labels, digits=4, labels=labels))\n",
        "    print(\"Report %s:\"%t, classification_report(actual_labels, pred_labels, digits=4))\n",
        "    f1_score(y_pred=pred_labels, y_true=actual_labels, average='micro')\n",
        "    p,r,f,_ = precision_recall_fscore_support(y_pred=pred_labels, y_true=actual_labels)\n",
        "    results = dict()\n",
        "    results[\"f1 score\"] = f[1]\n",
        "    results[\"recall\"] = r[1]\n",
        "    results[\"precision\"] = p[1]\n",
        "    results[\"specificity\"] = r[0]     \n",
        "    print(t, results) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report mednli:                precision    recall  f1-score   support\n",
            "\n",
            "CONTRADICTION     0.8929    0.8966    0.8947       474\n",
            "   ENTAILMENT     0.8148    0.8354    0.8250       474\n",
            "      NEUTRAL     0.8391    0.8143    0.8266       474\n",
            "\n",
            "     accuracy                         0.8488      1422\n",
            "    macro avg     0.8489    0.8488    0.8488      1422\n",
            " weighted avg     0.8489    0.8488    0.8488      1422\n",
            "\n",
            "mednli {'f1 score': 0.825, 'recall': 0.8354430379746836, 'precision': 0.8148148148148148, 'specificity': 0.8966244725738397}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RtNyKYbGPOX"
      },
      "source": [
        "euadr_10 {'f1 score': 0.9923076923076923, 'recall': 0.9847328244274809, 'precision': 1.0, 'specificity': 1.0}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91cyP4YPJFU",
        "outputId": "a758f7cb-4cf8-449d-8373-57d3581194dc"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "question_1 = \"Where is the Google headquarters located?\" \n",
        "# question_2 = \"What is the most populous country in the world?\" \n",
        "# question_3 = \"Who are the 4 members of The Beatles?\" \n",
        "# question_4 = \"How many teeth do humans have?\"\n",
        "\n",
        "questions = [question_1]\n",
        "\n",
        "now = time.time()\n",
        "# Write out the supplied questions to text files.\n",
        "predict_inputs_path = os.path.join(MODEL_DIR, \"predict_inputs_%d.txt\" % now)\n",
        "predict_outputs_path = os.path.join(MODEL_DIR, \"predict_outputs_%d.txt\" % now)\n",
        "# Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "\n",
        "with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n",
        "  for q in questions:\n",
        "    f.write(\"chemprot_re: %s\\n\" % q.lower())\n",
        "\n",
        "# Ignore any logging so that we only see the model's answers to the questions.\n",
        "with tf_verbosity_level('ERROR'):\n",
        "  model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "  model.predict(\n",
        "      input_file=predict_inputs_path,\n",
        "      output_file=predict_outputs_path,\n",
        "      # Select the most probable output token at each step.\n",
        "      temperature=0,\n",
        "  )\n",
        "\n",
        "# The output filename will have the checkpoint appended so we glob to get \n",
        "# the latest.\n",
        "prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
        "  for q, a in zip(questions, f):\n",
        "    if q:\n",
        "      print(\"Q: \" + q)\n",
        "      print(\"A: \" + a)\n",
        "      print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Predictions using checkpoint 1242600:\n",
            "\n",
            "Q: Where is the Google headquarters located?\n",
            "A: \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV6wVrYgSqu_",
        "outputId": "22b3b093-dcba-4bd2-fe09-46a6bd2ef738"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1rd2Tc6oUWBo7JouwexW3ksQ0PaOhUr6h\n",
        "!unzip Cleaned_CodeSearchNet.zip\n",
        "!rm Cleaned_CodeSearchNet.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rd2Tc6oUWBo7JouwexW3ksQ0PaOhUr6h\n",
            "To: /content/Cleaned_CodeSearchNet.zip\n",
            "381MB [00:03, 122MB/s]\n",
            "Archive:  Cleaned_CodeSearchNet.zip\n",
            "   creating: CodeSearchNet/\n",
            "   creating: CodeSearchNet/ruby/\n",
            "  inflating: CodeSearchNet/ruby/valid.jsonl  \n",
            "  inflating: CodeSearchNet/ruby/train.jsonl  \n",
            "  inflating: CodeSearchNet/ruby/test.jsonl  \n",
            "   creating: CodeSearchNet/php/\n",
            "  inflating: CodeSearchNet/php/valid.jsonl  \n",
            "  inflating: CodeSearchNet/php/train.jsonl  \n",
            "  inflating: CodeSearchNet/php/test.jsonl  \n",
            "   creating: CodeSearchNet/python/\n",
            "  inflating: CodeSearchNet/python/valid.jsonl  \n",
            "  inflating: CodeSearchNet/python/train.jsonl  \n",
            "  inflating: CodeSearchNet/python/test.jsonl  \n",
            "   creating: CodeSearchNet/java/\n",
            "  inflating: CodeSearchNet/java/valid.jsonl  \n",
            "  inflating: CodeSearchNet/java/train.jsonl  \n",
            "  inflating: CodeSearchNet/java/test.jsonl  \n",
            "   creating: CodeSearchNet/go/\n",
            "  inflating: CodeSearchNet/go/valid.jsonl  \n",
            "  inflating: CodeSearchNet/go/train.jsonl  \n",
            "  inflating: CodeSearchNet/go/test.jsonl  \n",
            "   creating: CodeSearchNet/javascript/\n",
            "  inflating: CodeSearchNet/javascript/valid.jsonl  \n",
            "  inflating: CodeSearchNet/javascript/train.jsonl  \n",
            "  inflating: CodeSearchNet/javascript/test.jsonl  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT9A2za6TkoL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}