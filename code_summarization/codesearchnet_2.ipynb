{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codesearchnet_1",
      "provenance": [],
      "collapsed_sections": [
        "9xtIvOasl7CI",
        "pt5jaNDnLMF4",
        "gfw_jz9uLLy2",
        "1tZl55kWLK3y",
        "U-JgSsuel7EC",
        "BFoA73ouMBM1",
        "gAqbaQ7Ol7EK",
        "3y7-5c2Gl7EO",
        "z-vSwM9Tl7EQ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPd3O1Xl7CA"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Ar53sBl7CB"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVvzNA7l7CB",
        "outputId": "a02b1829-d4b4-4885-eb80-99e6fa817e73"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K     |████████████████████████████████| 235kB 5.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 20.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 16.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 46.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1CKuTdVl7CE",
        "outputId": "029f77bd-834c-4eab-c3f9-bd95d7a47269"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v2-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up GCS access...\n",
            "Running on TPU: grpc://10.65.72.234:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A969X_jAr2qE",
        "outputId": "2934719b-c7a1-47e5-8ece-4086c05e99d8"
      },
      "source": [
        "print(t5.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtIvOasl7CI"
      },
      "source": [
        "## Register codesearchnet Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt5jaNDnLMF4"
      },
      "source": [
        "### java\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCs8MoxiLMF4",
        "outputId": "9b3d2c4d-241e-49aa-af69-195a41f39c9a"
      },
      "source": [
        "def dumping_dataset_java(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/java/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"java: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_java(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public ImageSource apply ( ImageSource input )  OPEN_CURLY_TOKEN  final int  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  pixelMatrix = new int  OPEN_SQUARE_TOKEN  3  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  3  CLOSE_SQUARE_TOKEN  ; int w = input . getWidth ( ) ; int h = input . getHeight ( ) ; int  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  output = new int  OPEN_SQUARE_TOKEN  h  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  w  CLOSE_SQUARE_TOKEN  ; for ( int j = 1 ; j  SMALLER_TOKEN  h - 1 ; j ++ )  OPEN_CURLY_TOKEN  for ( int i = 1 ; i  SMALLER_TOKEN  w - 1 ; i ++ )  OPEN_CURLY_TOKEN  pixelMatrix  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  = input . getR ( i - 1 , j - 1 ) ; pixelMatrix  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN  = input . getRGB ( i - 1 , j ) ; pixelMatrix  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN  = input . getRGB ( i - 1 , j + 1 ) ; pixelMatrix  OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  = input . getRGB ( i , j - 1 ) ; pixelMatrix  OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN  = input . getRGB ( i , j + 1 ) ; pixelMatrix  OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  = input . getRGB ( i + 1 , j - 1 ) ; pixelMatrix  OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN  = input . getRGB ( i + 1 , j ) ; pixelMatrix  OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  2  CLOSE_SQUARE_TOKEN  = input . getRGB ( i + 1 , j + 1 ) ; int edge = ( int ) convolution ( pixelMatrix ) ; int rgb = ( edge  SMALLER_TOKEN  SMALLER_TOKEN  16 | edge  SMALLER_TOKEN  SMALLER_TOKEN  8 | edge ) ; output  OPEN_SQUARE_TOKEN  j  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  i  CLOSE_SQUARE_TOKEN  = rgb ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  MatrixSource source = new MatrixSource ( output ) ; return source ;  CLOSE_CURLY_TOKEN ', 'target': b'Expects a height mat as input'}\n",
            "{'input': b'public  SMALLER_TOKEN  L extends Listener  GREATER_TOKEN  void popEvent ( Event  SMALLER_TOKEN  ? , L  GREATER_TOKEN  expected )  OPEN_CURLY_TOKEN  synchronized ( this . stack )  OPEN_CURLY_TOKEN  final Event  SMALLER_TOKEN  ? , ?  GREATER_TOKEN  actual = this . stack . pop ( ) ; if ( actual != expected )  OPEN_CURLY_TOKEN  throw new IllegalStateException ( String . format ( \"Unbalanced pop: expected \\'%s\\' but encountered \\'%s\\'\" , expected . getListenerClass ( ) , actual ) ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'target': b'Pops the top event off the current event stack . This action has to be performed immediately after the event has been dispatched to all listeners .'}\n",
            "{'input': b'protected void modify ( Transaction t )  OPEN_CURLY_TOKEN  try  OPEN_CURLY_TOKEN  this . lock . writeLock ( ) . lock ( ) ; t . perform ( ) ;  CLOSE_CURLY_TOKEN  finally  OPEN_CURLY_TOKEN  this . lock . writeLock ( ) . unlock ( ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'target': b'Executes the given transaction within the context of a write lock .'}\n",
            "{'input': b'protected  SMALLER_TOKEN  E  GREATER_TOKEN  E read ( Supplier  SMALLER_TOKEN  E  GREATER_TOKEN  sup )  OPEN_CURLY_TOKEN  try  OPEN_CURLY_TOKEN  this . lock . readLock ( ) . lock ( ) ; return sup . get ( ) ;  CLOSE_CURLY_TOKEN  finally  OPEN_CURLY_TOKEN  this . lock . readLock ( ) . unlock ( ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'target': b'Executes the given supplier within the context of a read lock .'}\n",
            "{'input': b'protected void setOffsetAndLength ( long offset , int length ) throws IOException  OPEN_CURLY_TOKEN  this . offset = offset ; this . length = length ; this . position = 0 ; if ( subStream . position ( ) != offset )  OPEN_CURLY_TOKEN  subStream . seek ( offset ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'target': b'This should be called from a subclass constructor if offset or length are unknown at a time when SubIIMInputStream constructor is called . This method shouldn t be called more than once .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57ip-NtLMF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b30cf4-24ce-4711-f522-55f3fd64b5b6"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_java,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f43be582790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfw_jz9uLLy2"
      },
      "source": [
        "### php\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geh3lKLGLLy2",
        "outputId": "ddd03072-5ea9-4130-d932-b1db3b971c4d"
      },
      "source": [
        "def dumping_dataset_php(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/php/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"php: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_php(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b\"public function onChannelPreDelete ( ResourceControllerEvent  DOLLAR_TOKEN  event ) : void  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  channel =  DOLLAR_TOKEN  event - GREATER_TOKEN  getSubject ( ) ; if ( !  DOLLAR_TOKEN  channel instanceof ChannelInterface )  OPEN_CURLY_TOKEN  throw new UnexpectedTypeException (  DOLLAR_TOKEN  channel , ChannelInterface :: class ) ;  CLOSE_CURLY_TOKEN   DOLLAR_TOKEN  results =  DOLLAR_TOKEN  this - GREATER_TOKEN  channelRepository - GREATER_TOKEN  findBy (  OPEN_SQUARE_TOKEN  'enabled' = GREATER_TOKEN  true  CLOSE_SQUARE_TOKEN  ) ; if ( !  DOLLAR_TOKEN  results || ( count (  DOLLAR_TOKEN  results ) === 1 && current (  DOLLAR_TOKEN  results ) ===  DOLLAR_TOKEN  channel ) )  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  event - GREATER_TOKEN  stop ( 'sylius.channel.delete_error' ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN \", 'target': b'Prevent channel deletion if no more channels enabled .'}\n",
            "{'input': b'public function getTaxTotal ( ) : int  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  taxTotal = 0 ; foreach (  DOLLAR_TOKEN  this - GREATER_TOKEN  getAdjustments ( AdjustmentInterface :: TAX_ADJUSTMENT ) as  DOLLAR_TOKEN  taxAdjustment )  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  taxTotal +=  DOLLAR_TOKEN  taxAdjustment - GREATER_TOKEN  getAmount ( ) ;  CLOSE_CURLY_TOKEN  foreach (  DOLLAR_TOKEN  this - GREATER_TOKEN  units as  DOLLAR_TOKEN  unit )  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  taxTotal +=  DOLLAR_TOKEN  unit - GREATER_TOKEN  getTaxTotal ( ) ;  CLOSE_CURLY_TOKEN  return  DOLLAR_TOKEN  taxTotal ;  CLOSE_CURLY_TOKEN ', 'target': b'Returns sum of neutral and non neutral tax adjustments on order item and total tax of units .'}\n",
            "{'input': b'private function isLastEnabledEntity (  DOLLAR_TOKEN  result ,  DOLLAR_TOKEN  entity ) : bool  OPEN_CURLY_TOKEN  return !  DOLLAR_TOKEN  result || 0 === count (  DOLLAR_TOKEN  result ) || ( 1 === count (  DOLLAR_TOKEN  result ) &&  DOLLAR_TOKEN  entity === (  DOLLAR_TOKEN  result instanceof \\\\ Iterator ?  DOLLAR_TOKEN  result - GREATER_TOKEN  current ( ) : current (  DOLLAR_TOKEN  result ) ) ) ;  CLOSE_CURLY_TOKEN ', 'target': b'If no entity matched the query criteria or a single entity matched which is the same as the entity being validated the entity is the last enabled entity available .'}\n",
            "{'input': b'protected function recalculateTotal ( ) : void  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  this - GREATER_TOKEN  total =  DOLLAR_TOKEN  this - GREATER_TOKEN  itemsTotal +  DOLLAR_TOKEN  this - GREATER_TOKEN  adjustmentsTotal ; if (  DOLLAR_TOKEN  this - GREATER_TOKEN  total  SMALLER_TOKEN  0 )  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  this - GREATER_TOKEN  total = 0 ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'target': b'Items total + Adjustments total .'}\n",
            "{'input': b\"public function loginAction ( Request  DOLLAR_TOKEN  request ) : Response  OPEN_CURLY_TOKEN   DOLLAR_TOKEN  authenticationUtils =  DOLLAR_TOKEN  this - GREATER_TOKEN  get ( 'security.authentication_utils' ) ;  DOLLAR_TOKEN  error =  DOLLAR_TOKEN  authenticationUtils - GREATER_TOKEN  getLastAuthenticationError ( ) ;  DOLLAR_TOKEN  lastUsername =  DOLLAR_TOKEN  authenticationUtils - GREATER_TOKEN  getLastUsername ( ) ;  DOLLAR_TOKEN  options =  DOLLAR_TOKEN  request - GREATER_TOKEN  attributes - GREATER_TOKEN  get ( '_sylius' ) ;  DOLLAR_TOKEN  template =  DOLLAR_TOKEN  options  OPEN_SQUARE_TOKEN  'template'  CLOSE_SQUARE_TOKEN  ?? null ; Assert :: notNull (  DOLLAR_TOKEN  template , 'Template is not configured.' ) ;  DOLLAR_TOKEN  formType =  DOLLAR_TOKEN  options  OPEN_SQUARE_TOKEN  'form'  CLOSE_SQUARE_TOKEN  ?? UserLoginType :: class ;  DOLLAR_TOKEN  form =  DOLLAR_TOKEN  this - GREATER_TOKEN  get ( 'form.factory' ) - GREATER_TOKEN  createNamed ( '' ,  DOLLAR_TOKEN  formType ) ; return  DOLLAR_TOKEN  this - GREATER_TOKEN  render (  DOLLAR_TOKEN  template ,  OPEN_SQUARE_TOKEN  'form' = GREATER_TOKEN   DOLLAR_TOKEN  form - GREATER_TOKEN  createView ( ) , 'last_username' = GREATER_TOKEN   DOLLAR_TOKEN  lastUsername , 'error' = GREATER_TOKEN   DOLLAR_TOKEN  error ,  CLOSE_SQUARE_TOKEN  ) ;  CLOSE_CURLY_TOKEN \", 'target': b'Login form action .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q39Rl4-QLLy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a0965e-bdbb-4247-e2da-b4618344ac54"
      },
      "source": [
        "t5.data.TaskRegistry.remove('php')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"php\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_php,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f43be592f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ujnPGvLLLkI"
      },
      "source": [
        "### js\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkRMCkgXLLkI",
        "outputId": "db387787-a893-47aa-940b-0b151132a942"
      },
      "source": [
        "def dumping_dataset_js(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/javascript/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    # print('text', text)\n",
        "\n",
        "    text = tf.strings.regex_replace(text, \"\\<\", \"SMALLER_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\>\", \"GREATER_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\[\", \"OPEN_SQUARE_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\]\", \"CLOSE_SQUARE_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\{\", \"OPEN_CURLY_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\}\", \"CLOSE_CURLY_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\^\", \"EXPONENTIAL_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\#\", \"SHARP_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\$\", \"DOLLAR_TOKEN\")\n",
        "    text = tf.strings.regex_replace(text, \"\\`\", \"UNK_TOKEN\")\n",
        "\n",
        "    return text\n",
        "    \n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "\n",
        "\n",
        "             tf.strings.join(\n",
        "                 [\"javascript: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_js(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'function ( state , action )  OPEN_CURLY_TOKEN  return _ . defaults (  OPEN_CURLY_TOKEN  isValidating : action . isValidating , lastAction : IS_VALIDATING  CLOSE_CURLY_TOKEN  , state )  CLOSE_CURLY_TOKEN ', 'target': b'Update is validating result'}\n",
            "{'input': b'function addWidgetForFilter ( view , filter , editModeHint )  OPEN_CURLY_TOKEN  var gridster = view . _widgetsGridster ; var row = filter . row || 1 ; var col = filter . col || 1 ; var sizeX = filter . size_x || 3 ; var sizeY = filter . size_y || 3 ; var el = gridster . add_widget ( \\' SMALLER_TOKEN div class=\"widgetOuterFrame\" GREATER_TOKEN  SMALLER_TOKEN /div GREATER_TOKEN \\' , sizeX , sizeY , col , row ) ; var frameView = new WidgetFrameView (  OPEN_CURLY_TOKEN  model : filter  CLOSE_CURLY_TOKEN  ) ; view . renderSubview ( frameView , el  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  ) ; frameView . renderContent ( ) ; frameView . gridsterHook = el  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  ;  DOLLAR_TOKEN  ( el  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  ) . data ( \\'spotWidgetFrameView\\' , frameView ) ; var chartView = frameView . widget ; chartView . model . updateConfiguration ( ) ; if ( chartView . model . isConfigured )  OPEN_CURLY_TOKEN  if ( ! filter . isInitialized )  OPEN_CURLY_TOKEN  filter . initDataFilter ( ) ;  CLOSE_CURLY_TOKEN  if ( ! chartView . isInitialized )  OPEN_CURLY_TOKEN  chartView . initChart ( ) ;  CLOSE_CURLY_TOKEN  chartView . update ( ) ; frameView . editMode = editModeHint ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  frameView . editMode = true ;  CLOSE_CURLY_TOKEN  filter . on ( \\'newData\\' , function ( )  OPEN_CURLY_TOKEN  chartView . update ( ) ;  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN ', 'target': b'Add a widget to the analyze page for the given filter'}\n",
            "{'input': b\"function inRange ( value , min , max )  OPEN_CURLY_TOKEN  const int = parseInt ( value , 10 ) return (  UNK_TOKEN   DOLLAR_TOKEN  OPEN_CURLY_TOKEN  int  CLOSE_CURLY_TOKEN   UNK_TOKEN  ===  UNK_TOKEN   DOLLAR_TOKEN  OPEN_CURLY_TOKEN  value . replace ( /  EXPONENTIAL_TOKEN 0 / , '' )  CLOSE_CURLY_TOKEN   UNK_TOKEN  && int  GREATER_TOKEN = min && int  SMALLER_TOKEN = max )  CLOSE_CURLY_TOKEN \", 'target': b'Determine if value is within a numeric range'}\n",
            "{'input': b\"function markdown ( options )  OPEN_CURLY_TOKEN  return new Remarkable ( extend (  OPEN_CURLY_TOKEN  breaks : false , html : true , langPrefix : 'lang-' , linkify : true , typographer : false , xhtmlOut : false  CLOSE_CURLY_TOKEN  , options ) ) ;  CLOSE_CURLY_TOKEN \", 'target': b'Shared settings for remarkable'}\n",
            "{'input': b\"function partitionValueToIndex ( partition , value )  OPEN_CURLY_TOKEN  var group ; if ( ! partition )  OPEN_CURLY_TOKEN  return 0 ;  CLOSE_CURLY_TOKEN  group = partition . groups . get ( value , 'value' ) ; if ( group )  OPEN_CURLY_TOKEN  return group . groupIndex ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  return - 1 ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN \", 'target': b'Get the index in chartjs datastructures from the group value with proper fallbacks'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy-_Utt1LLkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a55acd2-acbe-4332-d644-90ee0bffc863"
      },
      "source": [
        "t5.data.TaskRegistry.remove('js')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"js\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_js,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f438b8df110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WeBFoyI1MOU",
        "outputId": "2f44b845-2f80-4726-b657-b91555675e24"
      },
      "source": [
        "dumping_dataset = t5.data.TaskRegistry.get(\"js\")\n",
        "ds = dumping_dataset.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 128})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(20)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed validation examples...\n",
            "{'inputs_pretokenized': b\"javascript: function normalize ( obj , caseType = 'camel' )  OPEN_CURLY_TOKEN  let ret = obj ; const method = methods  OPEN_SQUARE_TOKEN  caseType  CLOSE_SQUARE_TOKEN  ; if ( Array . isArray ( obj ) )  OPEN_CURLY_TOKEN  ret =  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  ; let i = 0 ; while ( i  SMALLER_TOKEN  obj . length )  OPEN_CURLY_TOKEN  ret . push ( normalize ( obj  OPEN_SQUARE_TOKEN  i  CLOSE_SQUARE_TOKEN  , caseType ) ) ; ++ i ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  else if ( isPlainObject ( obj ) )  OPEN_CURLY_TOKEN  ret =  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ; for ( const k in obj )  OPEN_CURLY_TOKEN  ret  OPEN_SQUARE_TOKEN  method ( k )  CLOSE_SQUARE_TOKEN  = normalize ( obj  OPEN_SQUARE_TOKEN  k  CLOSE_SQUARE_TOKEN  , caseType ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  return ret ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,  1389,  1737,    41,     3,\n",
            "          32,   115,   354,     3,     6,   495, 25160,  3274,     3,\n",
            "          31,   658,  2341,    31,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,   752,     3,\n",
            "          60,    17,  3274,     3,    32,   115,   354,     3,   117,\n",
            "        6900,    17,  1573,  3274,  2254,   411, 20702,   834,   134,\n",
            "       16892,  7451,   834,  5647,   439,  5332,   495, 25160, 11175,\n",
            "       22177,   834,   134, 16892,  7451,   834,  5647,   439,  5332,\n",
            "           3,   117,     3,    99,    41,     3, 30652,     3,     5,\n",
            "          19, 30652,    41,     3,    32,   115,   354,     3,    61,\n",
            "           3,    61,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,     3,    60,    17,  3274,   411, 20702,\n",
            "         834,   134, 16892,  7451,   834,  5647,   439,  5332, 11175,\n",
            "       22177,   834,   134, 16892,  7451,   834,  5647,   439,  5332,\n",
            "           3,     1], dtype=int32), 'targets_pretokenized': b'Normalize all keys of obj recursively .', 'targets': array([16612,  1737,    66,  9060,    13,     3,    32,   115,   354,\n",
            "           3,    60, 15983, 13830,     3,     5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function ( member , memberValue )  OPEN_CURLY_TOKEN  var attributeName = this . _extractAttributeName ( member ) ; if ( ! attributeName )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  this . _addToDefinitionAttributes ( attributeName , memberValue ) ; return true ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,    41,  1144,     3,     6,\n",
            "        1144, 18392,    76,    15,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,  4331,\n",
            "       15816, 23954,  3274,    48,     3,     5,     3,   834, 25666,\n",
            "          75,    17,   188,    17,  5135,    17,    15, 23954,    41,\n",
            "        1144,     3,    61,     3,   117,     3,    99,    41,     3,\n",
            "          55, 15816, 23954,     3,    61,   411, 20702,   834,   254,\n",
            "       21274,   476,   834,  5647,   439,  5332,  1205,  6136,     3,\n",
            "         117, 11175, 22177,   834,   254, 21274,   476,   834,  5647,\n",
            "         439,  5332,    48,     3,     5,     3,   834, 13039,  3696,\n",
            "       16196,    77,  4749,   188,    17,  5135,  1422,    41, 15816,\n",
            "       23954,     3,     6,  1144, 18392,    76,    15,     3,    61,\n",
            "           3,   117,  1205,  1176,     3,   117, 11175, 22177,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'Check if the current member is an attribute declaration . If so stores it into a temporary map that will be processed as the last step .', 'targets': array([ 1972,     3,    99,     8,   750,  1144,    19,    46, 15816,\n",
            "       18165,     3,     5,   156,    78,  3253,    34,   139,     3,\n",
            "           9,  7234,  2828,    24,    56,    36,  8534,    38,     8,\n",
            "         336,  1147,     3,     5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function commentStrip ( string )  OPEN_CURLY_TOKEN  string = string . replace ( RE_COMMENT_SINGLE_LINE , '' ) ; string = string . replace ( RE_COMMENT_MULTI_LINES , '' ) ; return string ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,  1670,   134, 14192,    41,\n",
            "        6108,     3,    61,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,  6108,  3274,  6108,     3,     5,\n",
            "        3601,    41,  4083,   834,  6657, 11810,   834,   134,  2365,\n",
            "        3765,   834, 20006,     3,     6,     3,    31,    31,     3,\n",
            "          61,     3,   117,  6108,  3274,  6108,     3,     5,  3601,\n",
            "          41,  4083,   834,  6657, 11810,   834,   329,  4254,  5494,\n",
            "         834, 20006,   134,     3,     6,     3,    31,    31,     3,\n",
            "          61,     3,   117,  1205,  6108,     3,   117, 11175, 22177,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'Strip comments from string', 'targets': array([16209,  2622,    45,  6108,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function removeKeybindings ( bindings , method )  OPEN_CURLY_TOKEN  if ( ! bindings ) return ; if ( typeof bindings === 'string' && ! method ) console . warn ( 'removeKeybindings requries method as second arg' ) ; if ( typeof bindings === 'string' ) return removeMethod ( bindings , method ) ; Object . keys ( bindings ) . forEach ( ( key ) = GREATER_TOKEN  removeMethod ( key , bindings  OPEN_SQUARE_TOKEN  key  CLOSE_SQUARE_TOKEN  ) ) ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,  2036, 23617,  8610,    53,\n",
            "           7,    41, 11293,     7,     3,     6,  1573,     3,    61,\n",
            "         411, 20702,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332,     3,    99,    41,     3,    55, 11293,     7,     3,\n",
            "          61,  1205,     3,   117,     3,    99,    41,   686,   858,\n",
            "       11293,     7,  3274,  2423,  2423,     3,    31, 16099,    31,\n",
            "           3,   184,   184,     3,    55,  1573,     3,    61,  8990,\n",
            "           3,     5, 17640,    41,     3,    31,    60,  7168,    15,\n",
            "       23617,  8610,    53,     7,     3,    60,  2436,  2593,  1573,\n",
            "          38,   511,     3,  8240,    31,     3,    61,     3,   117,\n",
            "           3,    99,    41,   686,   858, 11293,     7,  3274,  2423,\n",
            "        2423,     3,    31, 16099,    31,     3,    61,  1205,  2036,\n",
            "       23351,   107,    32,    26,    41, 11293,     7,     3,     6,\n",
            "        1573,     3,    61,     3,   117,     3, 17057,     3,     5,\n",
            "        9060,     1], dtype=int32), 'targets_pretokenized': b'- Function to call to remove keybindings -', 'targets': array([    3,    18, 21839,    12,   580,    12,  2036,   843,  8610,\n",
            "          53,     7,     3,    18,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function checkIfSentvaluesAreSufficient ( req , dbField )  OPEN_CURLY_TOKEN  if ( dbField . Default == \\'FILE\\' )  OPEN_CURLY_TOKEN  if ( req . files . hasOwnProperty ( dbField . Field ) )  OPEN_CURLY_TOKEN  var file = req . files  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  . hasOwnProperty ( \\'name\\' ) ? req . files  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  : req . files  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN   OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  ; if ( settings . maxFileSize !== - 1 && file . size  GREATER_TOKEN  settings . maxFileSize )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  return file . name ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  if ( req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  === null || typeof req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  == \"undefined\" )  OPEN_CURLY_TOKEN  return dbField . Null == \"YES\" ? null : false ;  CLOSE_CURLY_TOKEN  if ( ( dbField . Type . indexOf ( \"int\" ) != - 1 || dbField . Type . indexOf ( \"float\" ) != - 1 || dbField . Type . indexOf ( \"double\" ) != - 1 ) )  OPEN_CURLY_TOKEN  return ! isNaN ( req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  ) ? req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  : false ;  CLOSE_CURLY_TOKEN  else if ( typeof req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  === \\'string\\' )  OPEN_CURLY_TOKEN  return escape ( req . body  OPEN_SQUARE_TOKEN  dbField . Field  CLOSE_SQUARE_TOKEN  ) ;  CLOSE_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,   691,  5801,   134,   295,\n",
            "       12097,     7,   188,    60,   134,    76,    89, 17397,    41,\n",
            "           3,    60,  1824,     3,     6,     3,    26,   115,  3183,\n",
            "        8804,     3,    61,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,     3,    99,    41,     3,    26,\n",
            "         115,  3183,  8804,     3,     5,     3, 29509,  3274,  2423,\n",
            "           3,    31,   371, 20129,    31,     3,    61,   411, 20702,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332,     3,\n",
            "          99,    41,     3,    60,  1824,     3,     5,  2073,     3,\n",
            "           5,    65,   667,   210,    29,  3174,   883,    17,    63,\n",
            "          41,     3,    26,   115,  3183,  8804,     3,     5,  7257,\n",
            "           3,    61,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,     3,  4331,  1042,  3274,\n",
            "           3,    60,  1824,     3,     5,  2073,   411, 20702,   834,\n",
            "         134,     1], dtype=int32), 'targets_pretokenized': b'Check roughly if the provided value is sufficient for the database field', 'targets': array([ 1972, 10209,     3,    99,     8,   937,   701,    19,  6684,\n",
            "          21,     8,  3501,  1057,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function _gpfStreamSecureInstallProgressFlag ( constructor )  OPEN_CURLY_TOKEN  constructor . prototype  OPEN_SQUARE_TOKEN  _gpfStreamProgressRead  CLOSE_SQUARE_TOKEN  = false ; constructor . prototype  OPEN_SQUARE_TOKEN  _gpfStreamProgressWrite  CLOSE_SQUARE_TOKEN  = false ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,     3,   834,   122,   102,\n",
            "          89, 10770,   134,    15,  3663,    15,  1570,  9176,  3174,\n",
            "       10292,   371,  5430,    41,  6774,   127,     3,    61,   411,\n",
            "       20702,   834,   254, 21274,   476,   834,  5647,   439,  5332,\n",
            "        6774,   127,     3,     5, 14402,   411, 20702,   834,   134,\n",
            "       16892,  7451,   834,  5647,   439,  5332,     3,   834,   122,\n",
            "         102,    89, 10770,  3174, 10292, 19915, 11175, 22177,   834,\n",
            "         134, 16892,  7451,   834,  5647,   439,  5332,  3274,  6136,\n",
            "           3,   117,  6774,   127,     3,     5, 14402,   411, 20702,\n",
            "         834,   134, 16892,  7451,   834,  5647,   439,  5332,     3,\n",
            "         834,   122,   102,    89, 10770,  3174, 10292, 24965,    15,\n",
            "       11175, 22177,   834,   134, 16892,  7451,   834,  5647,   439,\n",
            "        5332,  3274,  6136,     3,   117, 11175, 22177,   834,   254,\n",
            "       21274,   476,   834,  5647,   439,  5332,     1], dtype=int32), 'targets_pretokenized': b'Install the progress flag used by _gpfStreamSecureRead and Write', 'targets': array([11985,     8,  2188,  5692,   261,    57,     3,   834,   122,\n",
            "         102,    89, 10770,   134,    15,  3663,    15, 19915,    11,\n",
            "        8733,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function _gpfWebTagCreateFunction ( nodeName )  OPEN_CURLY_TOKEN  if ( ! nodeName )  OPEN_CURLY_TOKEN  gpf . Error . missingNodeName ( ) ;  CLOSE_CURLY_TOKEN  return function ( firstParam )  OPEN_CURLY_TOKEN  var sliceFrom = 0 , attributes ; if ( _gpfIsLiteralObject ( firstParam ) )  OPEN_CURLY_TOKEN  attributes = firstParam ; ++ sliceFrom ;  CLOSE_CURLY_TOKEN  return new _GpfWebTag ( nodeName , attributes , _gpfArraySlice ( arguments , sliceFrom ) ) ;  CLOSE_CURLY_TOKEN  ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,     3,   834,   122,   102,\n",
            "          89, 15805, 23593,   254,    60,   342,   371,   202,  4985,\n",
            "          41,   150,   221, 23954,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,    99,\n",
            "          41,     3,    55,   150,   221, 23954,     3,    61,   411,\n",
            "       20702,   834,   254, 21274,   476,   834,  5647,   439,  5332,\n",
            "           3,   122,   102,    89,     3,     5,   848,    52,   127,\n",
            "           3,     5,  3586,  4168,   221, 23954,    41,     3,    61,\n",
            "           3,   117, 11175, 22177,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,  1205,  1681,    41,   166, 13212,   265,\n",
            "           3,    61,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,     3,  4331, 13810, 22674,  3274,     3,\n",
            "         632,     3,     6, 12978,     3,   117,     3,    99,    41,\n",
            "           3,   834,   122,   102,    89,   196,     7, 16278,  4900,\n",
            "       17057,     1], dtype=int32), 'targets_pretokenized': b'Create a tag generation function', 'targets': array([6357,    3,    9, 7860, 3381, 1681,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function partitionValueToIndex ( partition , value )  OPEN_CURLY_TOKEN  var group ; if ( ! partition )  OPEN_CURLY_TOKEN  return 0 ;  CLOSE_CURLY_TOKEN  group = partition . groups . get ( value , 'value' ) ; if ( group )  OPEN_CURLY_TOKEN  return group . groupIndex ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  return - 1 ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681, 16540, 18392,    76,    15,\n",
            "        3696, 26267,   226,    41, 16540,     3,     6,   701,     3,\n",
            "          61,   411, 20702,   834,   254, 21274,   476,   834,  5647,\n",
            "         439,  5332,     3,  4331,   563,     3,   117,     3,    99,\n",
            "          41,     3,    55, 16540,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,  1205,     3,\n",
            "         632,     3,   117, 11175, 22177,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,   563,  3274, 16540,     3,     5,\n",
            "        1637,     3,     5,   129,    41,   701,     3,     6,     3,\n",
            "          31, 12097,    31,     3,    61,     3,   117,     3,    99,\n",
            "          41,   563,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,  1205,   563,     3,     5,\n",
            "         563, 26267,   226,     3,   117, 11175, 22177,   834,   254,\n",
            "       21274,   476,   834,  5647,   439,  5332,  1307,   411, 20702,\n",
            "         834,     1], dtype=int32), 'targets_pretokenized': b'Get the index in chartjs datastructures from the group value with proper fallbacks', 'targets': array([ 1609,     8,  5538,    16,  5059,   354,     7,   331, 16180,\n",
            "           7,    45,     8,   563,   701,    28,  2757,  1590,  1549,\n",
            "           7,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function indexTranslations ( )  OPEN_CURLY_TOKEN  const  OPEN_CURLY_TOKEN  translations =  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  = getComponentsSettings ( ) ; return index (  OPEN_CURLY_TOKEN  file : 'translations.js' , config :  OPEN_CURLY_TOKEN  type : TYPE_TRANSLATIONS , config : translations ,  CLOSE_CURLY_TOKEN  , defaultContent : 'export default null; NEW_LINE ' ,  NEW_LINE  ,  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,  5538, 18474,  6105,     7,\n",
            "          41,     3,    61,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,  6900,    17,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,  7314,     7,\n",
            "        3274,   411, 20702,   834,   254, 21274,   476,   834,  5647,\n",
            "         439,  5332, 11175, 22177,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332, 11175, 22177,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,  3274,   129,  5890,  9977,     7,\n",
            "       17175,  1222,     7,    41,     3,    61,     3,   117,  1205,\n",
            "        5538,    41,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,  1042,     3,    10,     3,    31,  7031,\n",
            "        6105,     7,     5,   354,     7,    31,     3,     6,     3,\n",
            "       20303,     3,    10,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,   686,     3,    10,     3, 12016,\n",
            "        5668,     1], dtype=int32), 'targets_pretokenized': b'Indexes the translations from extensions .', 'targets': array([11507,    15,     7,     8,  7314,     7,    45, 15176,     3,\n",
            "           5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function renameIdentifier ( ast , oldName , newName , defNode )  OPEN_CURLY_TOKEN  const changes =  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  ; if ( ! defNode )  OPEN_CURLY_TOKEN  let scope ; traverse ( ast ,  OPEN_CURLY_TOKEN  Identifier ( path )  OPEN_CURLY_TOKEN  if ( path . node . name === oldName )  OPEN_CURLY_TOKEN  scope = path . scope ; path . stop ( ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ,  CLOSE_CURLY_TOKEN  ) ; if ( ! scope ) return changes ; defNode = getDefNode ( oldName , scope ) ;  CLOSE_CURLY_TOKEN  function rename ( path )  OPEN_CURLY_TOKEN  if ( path . node . name === oldName && path . key !== 'imported' && getDefNode ( path . node . name , path . scope ) === defNode )  OPEN_CURLY_TOKEN  path . node . name = newName ; changes . push (  OPEN_CURLY_TOKEN  start : path . node . start , end : path . node . end , replacement : newName ,  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  traverse ( ast ,  OPEN_CURLY_TOKEN  JSXIdentifier : rename , Identifier : rename ,  CLOSE_CURLY_TOKEN  ) ; return changes ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,     3,    60,  4350, 21153,\n",
            "        7903,    41,    38,    17,     3,     6,   625, 23954,     3,\n",
            "           6,   126, 23954,     3,     6,    20,    89,  4168,   221,\n",
            "           3,    61,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,  6900,    17,  1112,  3274,   411, 20702,\n",
            "         834,   134, 16892,  7451,   834,  5647,   439,  5332, 11175,\n",
            "       22177,   834,   134, 16892,  7451,   834,  5647,   439,  5332,\n",
            "           3,   117,     3,    99,    41,     3,    55,    20,    89,\n",
            "        4168,   221,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,   752,  7401,     3,   117,\n",
            "        5187,    15,    41,    38,    17,     3,     6,   411, 20702,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332,     3,\n",
            "       21153,  7903,    41,  2071,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,    99,\n",
            "          41,     1], dtype=int32), 'targets_pretokenized': b'Rename an top scope identifier in a module . If first finds the definition node of the given name . Then rename all identifiers those refer to that definition node .', 'targets': array([  419,  4350,    46,   420,  7401,     3,  8826,    52,    16,\n",
            "           3,     9,  6008,     3,     5,   156,   166, 12902,     8,\n",
            "        4903,   150,   221,    13,     8,   787,   564,     3,     5,\n",
            "          37,    29,     3,    60,  4350,    66,     3,  8826,    52,\n",
            "           7,   273,  2401,    12,    24,  4903,   150,   221,     3,\n",
            "           5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function ( data )  OPEN_CURLY_TOKEN  var me = this ; me . _waitForData ( ) ; me . _dataInResolve ( data ) ; return new Promise ( function ( resolve , reject )  OPEN_CURLY_TOKEN  me . _dataOutResolve = resolve ; me . _dataOutReject = reject ;  CLOSE_CURLY_TOKEN  ) . then ( function ( value )  OPEN_CURLY_TOKEN  delete me . _dataOutResolve ; delete me . _dataOutReject ; return value ;  CLOSE_CURLY_TOKEN  , function ( reason )  OPEN_CURLY_TOKEN  delete me . _dataOutResolve ; delete me . _dataOutReject ; return Promise . reject ( reason ) ;  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,    41,   331,     3,    61,\n",
            "         411, 20702,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332,     3,  4331,   140,  3274,    48,     3,   117,   140,\n",
            "           3,     5,     3,   834, 26745,  3809, 20367,    41,     3,\n",
            "          61,     3,   117,   140,     3,     5,     3,   834,  6757,\n",
            "        1570,  1649,  6065,    15,    41,   331,     3,    61,     3,\n",
            "         117,  1205,   126, 28844,    41,  1681,    41,  7785,     3,\n",
            "           6, 15092,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,   140,     3,     5,     3,\n",
            "         834,  6757, 15767,  1649,  6065,    15,  3274,  7785,     3,\n",
            "         117,   140,     3,     5,     3,   834,  6757, 15767,  1649,\n",
            "       11827,  3274, 15092,     3,   117, 11175, 22177,   834,   254,\n",
            "       21274,   476,   834,  5647,   439,  5332,     3,    61,     3,\n",
            "           5,   258,    41,  1681,    41,   701,     3,    61,   411,\n",
            "       20702,     1], dtype=int32), 'targets_pretokenized': b'Waits for the read API to write it out', 'targets': array([14583,     7,    21,     8,   608,  6429,    12,  1431,    34,\n",
            "          91,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function skipAtomic ( doc , pos , bias , mayClear )  OPEN_CURLY_TOKEN  var flipped = false , curPos = pos ; var dir = bias || 1 ; doc . cantEdit = false ; search : for ( ; ; )  OPEN_CURLY_TOKEN  var line = getLine ( doc , curPos . line ) ; if ( line . markedSpans )  OPEN_CURLY_TOKEN  for ( var i = 0 ; i  SMALLER_TOKEN  line . markedSpans . length ; ++ i )  OPEN_CURLY_TOKEN  var sp = line . markedSpans  OPEN_SQUARE_TOKEN  i  CLOSE_SQUARE_TOKEN  , m = sp . marker ; if ( ( sp . from == null || ( m . inclusiveLeft ? sp . from  SMALLER_TOKEN = curPos . ch : sp . from  SMALLER_TOKEN  curPos . ch ) ) && ( sp . to == null || ( m . inclusiveRight ? sp . to  GREATER_TOKEN = curPos . ch : sp . to  GREATER_TOKEN  curPos . ch ) ) )  OPEN_CURLY_TOKEN  if ( mayClear )  OPEN_CURLY_TOKEN  signal ( m , \"beforeCursorEnter\" ) ; if ( m . explicitlyCleared )  OPEN_CURLY_TOKEN  if ( ! line . markedSpans ) break ; else  OPEN_CURLY_TOKEN  -- i ; continue ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  if ( ! m . atomic ) continue ; var newPos = m . find ( dir  SMALLER_TOKEN  0 ? - 1 : 1 ) ; if ( cmp ( newPos , curPos ) == 0 )  OPEN_CURLY_TOKEN  newPos . ch += dir ; if ( newPos . ch  SMALLER_TOKEN  0 )  OPEN_CURLY_TOKEN  if ( newPos . line  GREATER_TOKEN  doc . first ) newPos = clipPos ( doc , Pos ( newPos . line - 1 ) ) ; else newPos = null ;  CLOSE_CURLY_TOKEN  else if ( newPos . ch  GREATER_TOKEN  line . text . length )  OPEN_CURLY_TOKEN  if ( newPos . line  SMALLER_TOKEN  doc . first + doc . size - 1 ) newPos = Pos ( newPos . line + 1 , 0 ) ; else newPos = null ;  CLOSE_CURLY_TOKEN  if ( ! newPos )  OPEN_CURLY_TOKEN  if ( flipped )  OPEN_CURLY_TOKEN  if ( ! mayClear ) return skipAtomic ( doc , pos , bias , true ) ; doc . cantEdit = true ; return Pos ( doc . first , 0 ) ;  CLOSE_CURLY_TOKEN  flipped = true ; newPos = pos ; dir = - dir ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  curPos = newPos ; continue search ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN  return curPos ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681, 11202,   188,   235,  3113,\n",
            "          41,     3,  7171,     3,     6,     3,  2748,     3,     6,\n",
            "       14387,     3,     6,   164,   254,   109,   291,     3,    61,\n",
            "         411, 20702,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332,     3,  4331,     3, 31553,  3274,  6136,     3,     6,\n",
            "        5495,   345,    32,     7,  3274,     3,  2748,     3,   117,\n",
            "           3,  4331,  5141,  3274, 14387,  1820,  9175,   209,     3,\n",
            "         117,     3,  7171,     3,     5,    54,    17, 26527,  3274,\n",
            "        6136,     3,   117,   960,     3,    10,    21,    41,     3,\n",
            "         117,     3,   117,     3,    61,   411, 20702,   834,   254,\n",
            "       21274,   476,   834,  5647,   439,  5332,     3,  4331,   689,\n",
            "        3274,   129, 21022,    41,     3,  7171,     3,     6,  5495,\n",
            "         345,    32,     7,     3,     5,   689,     3,    61,     3,\n",
            "         117,     3,    99,    41,   689,     3,     5,  7027, 19675,\n",
            "           7,     1], dtype=int32), 'targets_pretokenized': b'Ensure a given position is not inside an atomic range .', 'targets': array([    3, 18521,     3,     9,   787,  1102,    19,    59,  1096,\n",
            "          46,     3, 20844,   620,     3,     5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function _gpfFuncUnsafe ( params , source )  OPEN_CURLY_TOKEN  var args ; if ( ! params . length )  OPEN_CURLY_TOKEN  return _GpfFunc ( source ) ;  CLOSE_CURLY_TOKEN  args =  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  . concat ( params ) ; args . push ( source ) ; return _GpfFunc . apply ( null , args ) ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,     3,   834,   122,   102,\n",
            "          89,   371,   202,    75,  5110, 15233,    41,   260,   265,\n",
            "           7,     3,     6,  1391,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,  4331,\n",
            "           3,  8240,     7,     3,   117,     3,    99,    41,     3,\n",
            "          55,   260,   265,     7,     3,     5,  2475,     3,    61,\n",
            "         411, 20702,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332,  1205,     3,   834,   517,   102,    89,   371,   202,\n",
            "          75,    41,  1391,     3,    61,     3,   117, 11175, 22177,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332,     3,\n",
            "        8240,     7,  3274,   411, 20702,   834,   134, 16892,  7451,\n",
            "         834,  5647,   439,  5332, 11175, 22177,   834,   134, 16892,\n",
            "        7451,   834,  5647,   439,  5332,     3,     5,   975,  2138,\n",
            "          41,   260,   265,     7,     3,    61,     3,   117,     3,\n",
            "        8240,     1], dtype=int32), 'targets_pretokenized': b'Unprotected version of _gpfFunc', 'targets': array([  597, 19812,    15,    26,   988,    13,     3,   834,   122,\n",
            "         102,    89,   371,   202,    75,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function executable ( file )  OPEN_CURLY_TOKEN  try  OPEN_CURLY_TOKEN  fs . accessSync ( file , fs . X_OK ) ; return true ;  CLOSE_CURLY_TOKEN  catch ( e )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,  9362,   179,    41,  1042,\n",
            "           3,    61,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,   653,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,     3,    89,     7,     3,\n",
            "           5,   592, 24863,    41,  1042,     3,     6,     3,    89,\n",
            "           7,     3,     5,     3,     4,   834, 13458,     3,    61,\n",
            "           3,   117,  1205,  1176,     3,   117, 11175, 22177,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,  3579,    41,\n",
            "           3,    15,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,  1205,  6136,     3,   117,\n",
            "       11175, 22177,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332, 11175, 22177,   834,   254, 21274,   476,   834,  5647,\n",
            "         439,  5332,     1], dtype=int32), 'targets_pretokenized': b'Check whether a file is executable or not', 'targets': array([1972,  823,    3,    9, 1042,   19, 9362,  179,   42,   59,    1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function removeFromArray ( ast , varName , identifierName )  OPEN_CURLY_TOKEN  let changes =  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  ; traverse ( ast ,  OPEN_CURLY_TOKEN  VariableDeclarator ( path )  OPEN_CURLY_TOKEN  const node = path . node ; if ( _ . get ( node , 'id.name' ) !== varName || _ . get ( node , 'init.type' ) !== 'ArrayExpression' ) return ; node . init . _filePath = ast . _filePath ; const toRemove = _ . find ( node . init . elements , ele = GREATER_TOKEN  ele . name === identifierName ) ; changes = removeFromArrayByNode ( node . init , toRemove ) ; path . stop ( ) ;  CLOSE_CURLY_TOKEN  ,  CLOSE_CURLY_TOKEN  ) ; return changes ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,  2036, 22674, 30652,    41,\n",
            "          38,    17,     3,     6,     3,  4331, 23954,     3,     6,\n",
            "           3,  8826,    52, 23954,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,   752,  1112,\n",
            "        3274,   411, 20702,   834,   134, 16892,  7451,   834,  5647,\n",
            "         439,  5332, 11175, 22177,   834,   134, 16892,  7451,   834,\n",
            "        5647,   439,  5332,     3,   117,  5187,    15,    41,    38,\n",
            "          17,     3,     6,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332, 12928,   179,  2962, 23982,  1016,\n",
            "          41,  2071,     3,    61,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,  6900,    17,   150,   221,\n",
            "        3274,  2071,     3,     5,   150,   221,     3,   117,     3,\n",
            "          99,    41,     3,   834,     3,     5,   129,    41,   150,\n",
            "         221,     3,     6,     3,    31,    23,    26,     5,  4350,\n",
            "          31,     1], dtype=int32), 'targets_pretokenized': b'Remove an identifier from the array of the name varName . It only finds the first matched array in the global scope of ast .', 'targets': array([10002,    46,     3,  8826,    52,    45,     8,  5590,    13,\n",
            "           8,   564,     3,  4331, 23954,     3,     5,    94,   163,\n",
            "       12902,     8,   166,     3, 10304,  5590,    16,     8,  1252,\n",
            "        7401,    13,    38,    17,     3,     5,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function init ( opts )  OPEN_CURLY_TOKEN  opts = opts ||  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ; opts . pluginOptions = opts . pluginOptions ||  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ; opts . pluginOptions . pancakes = exports ; injector = new DependencyInjector ( opts ) ; var ClientPlugin = opts . clientPlugin ; var ServerPlugin = opts . serverPlugin ; if ( ClientPlugin )  OPEN_CURLY_TOKEN  clientPlugin = new ClientPlugin ( opts ) ;  CLOSE_CURLY_TOKEN  if ( ServerPlugin )  OPEN_CURLY_TOKEN  serverPlugin = new ServerPlugin ( opts ) ;  CLOSE_CURLY_TOKEN  apiRouteHandler . init (  OPEN_CURLY_TOKEN  injector : injector  CLOSE_CURLY_TOKEN  ) ; webRouteHandler . init (  OPEN_CURLY_TOKEN  injector : injector , clientPlugin : clientPlugin , rootDir : opts . rootDir  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,    16,   155,    41,  3882,\n",
            "           7,     3,    61,   411, 20702,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,  3882,     7,  3274,  3882,     7,\n",
            "        1820,  9175,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332, 11175, 22177,   834,   254, 21274,   476,\n",
            "         834,  5647,   439,  5332,     3,   117,  3882,     7,     3,\n",
            "           5,  7339,  9546,   106,     7,  3274,  3882,     7,     3,\n",
            "           5,  7339,  9546,   106,     7,  1820,  9175,   411, 20702,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332, 11175,\n",
            "       22177,   834,   254, 21274,   476,   834,  5647,   439,  5332,\n",
            "           3,   117,  3882,     7,     3,     5,  7339,  9546,   106,\n",
            "           7,     3,     5, 21365,     7,  3274,  4202,     7,     3,\n",
            "         117, 15823,   127,  3274,   126, 30718,  4392,  1570, 11827,\n",
            "         127,    41,  3882,     7,     3,    61,     3,   117,     3,\n",
            "        4331,     1], dtype=int32), 'targets_pretokenized': b'For now this is just a passthrough to the injector s init function', 'targets': array([  242,   230,    48,    19,   131,     3,     9,  1903, 11258,\n",
            "          12,     8, 15823,   127,     3,     7,    16,   155,  1681,\n",
            "           1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function checkRateRange ( num )  OPEN_CURLY_TOKEN  var numString = num . substring ( 0 , num . length - 1 ) ; var parseNum = parseInt ( numString ) ; if ( parseNum  SMALLER_TOKEN  20 )  OPEN_CURLY_TOKEN  throw new Error ( \"The minimum rate is twenty percentage. Received: \" + parseNum ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,   691,   448,   342,   448,\n",
            "        3280,    41,     3,  5525,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,  4331,\n",
            "           3,  5525, 11500,    53,  3274,     3,  5525,     3,     5,\n",
            "         769, 16099,    41,     3,   632,     3,     6,     3,  5525,\n",
            "           3,     5,  2475,     3,    18,   209,     3,    61,     3,\n",
            "         117,     3,  4331,   260,     7,    15,   567,   440,  3274,\n",
            "         260,     7,    15,  1570,    17,    41,     3,  5525, 11500,\n",
            "          53,     3,    61,     3,   117,     3,    99,    41,   260,\n",
            "           7,    15,   567,   440,     3,  4212, 12126,  3316,   834,\n",
            "        5647,   439,  5332,   460,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,  3793,   126,\n",
            "         848,    52,   127,    41,    96,   634,  2559,  1080,    19,\n",
            "        6786,  5294,     5, 24083,    26,    10,    96,  1768,   260,\n",
            "           7,     1], dtype=int32), 'targets_pretokenized': b'This method ensures that the value of the rate must be equal or great than 20%', 'targets': array([ 100, 1573,  766,    7,   24,    8,  701,   13,    8, 1080,  398,\n",
            "         36, 4081,   42,  248,  145, 7580,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function ( match )  OPEN_CURLY_TOKEN  var lengthOfMatchedString = match  OPEN_SQUARE_TOKEN  _GPF_START  CLOSE_SQUARE_TOKEN  . length , charAfterValue = this . _content . charAt ( lengthOfMatchedString ) ; if ( charAfterValue )  OPEN_CURLY_TOKEN  _gpfAssert ( charAfterValue === this . _separator , \"Positive lookahead works\" ) ; return this . _nextValue ( ++ lengthOfMatchedString ) ;  CLOSE_CURLY_TOKEN  delete this . _content ; return false ;  CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,    41,  1588,     3,    61,\n",
            "         411, 20702,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332,     3,  4331,  2475,   667,    89,   329,   144,  4513,\n",
            "       11500,    53,  3274,  1588,   411, 20702,   834,   134, 16892,\n",
            "        7451,   834,  5647,   439,  5332,     3,   834,  8049,   371,\n",
            "         834, 19481,   382, 11175, 22177,   834,   134, 16892,  7451,\n",
            "         834,  5647,   439,  5332,     3,     5,  2475,     3,     6,\n",
            "           3,  4059, 23901, 18392,    76,    15,  3274,    48,     3,\n",
            "           5,     3,   834, 14819,     3,     5,     3,  4059,   188,\n",
            "          17,    41,  2475,   667,    89,   329,   144,  4513, 11500,\n",
            "          53,     3,    61,     3,   117,     3,    99,    41,     3,\n",
            "        4059, 23901, 18392,    76,    15,     3,    61,   411, 20702,\n",
            "         834,   254, 21274,   476,   834,  5647,   439,  5332,     3,\n",
            "         834,   122,   102,    89,   188,     7,     7,    49,    17,\n",
            "          41,     1], dtype=int32), 'targets_pretokenized': b'Check what appears after the extracted value', 'targets': array([ 1972,   125,  3475,   227,     8, 21527,   701,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b\"javascript: function parsePlugins ( config )  OPEN_CURLY_TOKEN  let plugins =  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  ; function parse ( plugins )  OPEN_CURLY_TOKEN  return plugins . map ( plugin = GREATER_TOKEN   OPEN_CURLY_TOKEN  if ( 'string' == typeof plugin ) plugin = path . resolve ( plugin ) ; return plugin ;  CLOSE_CURLY_TOKEN  ) ;  CLOSE_CURLY_TOKEN  if ( config . plugins )  OPEN_CURLY_TOKEN  plugins . push ( ... parse ( config . plugins ) ) ; delete config . plugins ;  CLOSE_CURLY_TOKEN  if ( config . runtimeOptions . plugins )  OPEN_CURLY_TOKEN  plugins . push ( ... parse ( config . runtimeOptions . plugins ) ) ; delete config . runtimeOptions . plugins ;  CLOSE_CURLY_TOKEN  return plugins ;  CLOSE_CURLY_TOKEN \", 'inputs': array([    3, 27578, 11815,    10,  1681,   260,     7,    15, 23232,\n",
            "           7,    41,     3, 20303,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,   752,  7339,\n",
            "           7,  3274,   411, 20702,   834,   134, 16892,  7451,   834,\n",
            "        5647,   439,  5332, 11175, 22177,   834,   134, 16892,  7451,\n",
            "         834,  5647,   439,  5332,     3,   117,  1681,   260,     7,\n",
            "          15,    41,  7339,     7,     3,    61,   411, 20702,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,  1205,  7339,\n",
            "           7,     3,     5,  2828,    41,  7339,  3274, 26053,  3316,\n",
            "         834,  5647,   439,  5332,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,     3,    99,    41,     3,\n",
            "          31, 16099,    31,  3274,  2423,   686,   858,  7339,     3,\n",
            "          61,  7339,  3274,  2071,     3,     5,  7785,    41,  7339,\n",
            "           3,    61,     3,   117,  1205,  7339,     3,   117, 11175,\n",
            "       22177,     1], dtype=int32), 'targets_pretokenized': b'Parse plugins defined in config', 'targets': array([ 2180,     7,    15,  7339,     7,  4802,    16,     3, 20303,\n",
            "           1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'javascript: function ( modelAlias , fields )  OPEN_CURLY_TOKEN  var model = this . getTrackedModel ( modelAlias ) ; if ( model )  OPEN_CURLY_TOKEN  return  OPEN_CURLY_TOKEN  fields : fields , model : model  CLOSE_CURLY_TOKEN  ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN ', 'inputs': array([    3, 27578, 11815,    10,  1681,    41,   825,   188,    40,\n",
            "          23,     9,     7,     3,     6,  4120,     3,    61,   411,\n",
            "       20702,   834,   254, 21274,   476,   834,  5647,   439,  5332,\n",
            "           3,  4331,   825,  3274,    48,     3,     5,   129,   382,\n",
            "          52, 13365, 24663,    41,   825,   188,    40,    23,     9,\n",
            "           7,     3,    61,     3,   117,     3,    99,    41,   825,\n",
            "           3,    61,   411, 20702,   834,   254, 21274,   476,   834,\n",
            "        5647,   439,  5332,  1205,   411, 20702,   834,   254, 21274,\n",
            "         476,   834,  5647,   439,  5332,  4120,     3,    10,  4120,\n",
            "           3,     6,   825,     3,    10,   825, 11175, 22177,   834,\n",
            "         254, 21274,   476,   834,  5647,   439,  5332,     3,   117,\n",
            "       11175, 22177,   834,   254, 21274,   476,   834,  5647,   439,\n",
            "        5332, 11175, 22177,   834,   254, 21274,   476,   834,  5647,\n",
            "         439,  5332,     1], dtype=int32), 'targets_pretokenized': b'Returns a useful data structure that binds a tracked model to the fields being tracked on a mapping .', 'targets': array([ 9778,     7,     3,     9,  1934,   331,  1809,    24,     3,\n",
            "        8610,     7,     3,     9, 22679,   825,    12,     8,  4120,\n",
            "         271, 22679,    30,     3,     9, 14670,     3,     5,     1],\n",
            "      dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZl55kWLK3y"
      },
      "source": [
        "### ruby\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcjUh8YCLK35",
        "outputId": "d0757ed1-a443-47b9-ca4c-8716f9dfb74a"
      },
      "source": [
        "def dumping_dataset_ruby(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/ruby/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"ruby: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_ruby(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'def render_body ( context , options ) if options . key? ( :partial )  OPEN_SQUARE_TOKEN  render_partial ( context , options )  CLOSE_SQUARE_TOKEN  else StreamingTemplateRenderer . new ( @lookup_context ) . render ( context , options ) end end', 'target': b'Render but returns a valid Rack body . If fibers are defined we return a streaming body that renders the template piece by piece .'}\n",
            "{'input': b'def attribute_missing ( match , * args , & block ) __send__ ( match . target , match . attr_name , * args , & block ) end', 'target': b'+ attribute_missing + is like + method_missing + but for attributes . When + method_missing + is called we check to see if there is a matching attribute method . If so we tell + attribute_missing + to dispatch the attribute . This method can be overloaded to customize the behavior .'}\n",
            "{'input': b'def matched_attribute_method ( method_name ) matches = self . class . send ( :attribute_method_matchers_matching , method_name ) matches . detect  OPEN_CURLY_TOKEN  | match | attribute_method? ( match . attr_name )  CLOSE_CURLY_TOKEN  end', 'target': b'Returns a struct representing the matching attribute method . The struct s attributes are prefix base and suffix .'}\n",
            "{'input': b'def demodulize ( path ) path = path . to_s if i = path . rindex ( \"::\" ) path  OPEN_SQUARE_TOKEN  ( i + 2 ) .. - 1  CLOSE_SQUARE_TOKEN  else path end end', 'target': b'Removes the module part from the expression in the string .'}\n",
            "{'input': b'def const_regexp ( camel_cased_word ) parts = camel_cased_word . split ( \"::\" ) return Regexp . escape ( camel_cased_word ) if parts . blank? last = parts . pop parts . reverse . inject ( last ) do | acc , part | part . empty? ? acc : \" SHARP_TOKEN  OPEN_CURLY_TOKEN part CLOSE_CURLY_TOKEN (:: SHARP_TOKEN  OPEN_CURLY_TOKEN acc CLOSE_CURLY_TOKEN )?\" end end', 'target': b'Mounts a regular expression returned as a string to ease interpolation that will match part by part the given constant .'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWoF_w7ILK36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f0d3aa-de4a-4759-9a8d-5df6ecc6ab91"
      },
      "source": [
        "t5.data.TaskRegistry.remove('ruby')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"ruby\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_ruby,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f438846a850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JgSsuel7EC"
      },
      "source": [
        "### go\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJt-8jsUl7EC",
        "outputId": "8c4915c2-a0ae-4273-b01b-e5dc97bf621e"
      },
      "source": [
        "def dumping_dataset_go(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/go/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"go: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_go(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'func getAllDepTypes ( )  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  string  OPEN_CURLY_TOKEN  depTypes := make (  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  string , 0 , len ( cmds ) )  NEW_LINE  for depType := range cmds  OPEN_CURLY_TOKEN  depTypes = append ( depTypes , depType )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  sort . Strings ( depTypes )  NEW_LINE  return depTypes  NEW_LINE   CLOSE_CURLY_TOKEN ', 'target': b'getAllDepTypes returns a sorted list of names of all dep type commands .'}\n",
            "{'input': b'func getIoProgressReader ( label string , res * http . Response ) io . Reader  OPEN_CURLY_TOKEN  prefix := \"Downloading \" + label  NEW_LINE  fmtBytesSize := 18  NEW_LINE  barSize := int64 ( 80 - len ( prefix ) - fmtBytesSize )  NEW_LINE  bar := ioprogress . DrawTextFormatBarForW ( barSize , os . Stderr )  NEW_LINE  fmtfunc := func ( progress , total int64 ) string  OPEN_CURLY_TOKEN  if total == - 1  OPEN_CURLY_TOKEN  return fmt . Sprintf ( \"%s: %v of an unknown total size\" , prefix , ioprogress . ByteUnitStr ( progress ) , )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  return fmt . Sprintf ( \"%s: %s %s\" , prefix , bar ( progress , total ) , ioprogress . DrawTextFormatBytes ( progress , total ) , )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  return & ioprogress . Reader  OPEN_CURLY_TOKEN  Reader : res . Body , Size : res . ContentLength , DrawFunc : ioprogress . DrawTerminalf ( os . Stderr , fmtfunc ) , DrawInterval : time . Second ,  CLOSE_CURLY_TOKEN   NEW_LINE   CLOSE_CURLY_TOKEN ', 'target': b'getIoProgressReader returns a reader that wraps the HTTP response body so it prints a pretty progress bar when reading data from it .'}\n",
            "{'input': b'func ( f * removeOnClose ) Close ( ) error  OPEN_CURLY_TOKEN  if f == nil || f . File == nil  OPEN_CURLY_TOKEN  return nil  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  name := f . File . Name ( )  NEW_LINE  if err := f . File . Close ( ) ; err != nil  OPEN_CURLY_TOKEN  return err  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  if err := os . Remove ( name ) ; err != nil && ! os . IsNotExist ( err )  OPEN_CURLY_TOKEN  return err  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  return nil  NEW_LINE   CLOSE_CURLY_TOKEN ', 'target': b'Close closes the file and then removes it from disk . No error is returned if the file did not exist at the point of removal .'}\n",
            "{'input': b'func getTmpROC ( s * imagestore . Store , path string ) ( * removeOnClose , error )  OPEN_CURLY_TOKEN  h := sha512 . New ( )  NEW_LINE  h . Write (  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  byte ( path ) )  NEW_LINE  pathHash := s . HashToKey ( h )  NEW_LINE  tmp , err := s . TmpNamedFile ( pathHash )  NEW_LINE  if err != nil  OPEN_CURLY_TOKEN  return nil , errwrap . Wrap ( errors . New ( \"error setting up temporary file\" ) , err )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  _ , err = lock . TryExclusiveLock ( tmp . Name ( ) , lock . RegFile )  NEW_LINE  if err != nil  OPEN_CURLY_TOKEN  if err != lock . ErrLocked  OPEN_CURLY_TOKEN  return nil , errwrap . Wrap ( errors . New ( \"failed to lock temporary file\" ) , err )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  log . Printf ( \"another rkt instance is downloading this file, waiting...\" )  NEW_LINE  _ , err = lock . ExclusiveLock ( tmp . Name ( ) , lock . RegFile )  NEW_LINE  if err != nil  OPEN_CURLY_TOKEN  return nil , errwrap . Wrap ( errors . New ( \"failed to lock temporary file\" ) , err )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  return & removeOnClose  OPEN_CURLY_TOKEN  File : tmp  CLOSE_CURLY_TOKEN  , nil  NEW_LINE   CLOSE_CURLY_TOKEN ', 'target': b'getTmpROC returns a removeOnClose instance wrapping a temporary file provided by the passed store . The actual file name is based on a hash of the passed path .'}\n",
            "{'input': b'func getStage1Entrypoint ( cdir string , entrypoint string ) ( string , error )  OPEN_CURLY_TOKEN  b , err := ioutil . ReadFile ( common . Stage1ManifestPath ( cdir ) )  NEW_LINE  if err != nil  OPEN_CURLY_TOKEN  return \"\" , errwrap . Wrap ( errors . New ( \"error reading pod manifest\" ) , err )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  s1m := schema . ImageManifest  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN   NEW_LINE  if err := json . Unmarshal ( b , & s1m ) ; err != nil  OPEN_CURLY_TOKEN  return \"\" , errwrap . Wrap ( errors . New ( \"error unmarshaling stage1 manifest\" ) , err )  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  if ep , ok := s1m . Annotations . Get ( entrypoint ) ; ok  OPEN_CURLY_TOKEN  return ep , nil  NEW_LINE   CLOSE_CURLY_TOKEN   NEW_LINE  return \"\" , fmt . Errorf ( \"entrypoint %q not found\" , entrypoint )  NEW_LINE   CLOSE_CURLY_TOKEN ', 'target': b'getStage1Entrypoint retrieves the named entrypoint from the stage1 manifest for a given pod'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhK5bXycl7EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2459e47-a093-4a4b-b625-7cfebaa33c27"
      },
      "source": [
        "t5.data.TaskRegistry.remove('go')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"go\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_go,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f43882ddd50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFoA73ouMBM1"
      },
      "source": [
        "### python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnIhn9K_MBM-",
        "outputId": "94ed3eaa-84d4-4504-e5ef-8a7374fe1397"
      },
      "source": [
        "def dumping_dataset_python(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codesearchnet/python/train_encode.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"python: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_python(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  + level + result  OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN  . split ( \";\" )  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN ', 'target': b'Return either the full or truncated version of a QIIME - formatted taxonomy string .'}\n",
            "{'input': b'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )', 'target': b'Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .'}\n",
            "{'input': b'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle', 'target': b'Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .'}\n",
            "{'input': b'def gather_categories ( imap , header , categories = None ) : if categories is None : return  OPEN_CURLY_TOKEN  \"default\" : DataCategory ( set ( imap . keys ( ) ) ,  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  )  CLOSE_CURLY_TOKEN  cat_ids =  OPEN_SQUARE_TOKEN  header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat  CLOSE_SQUARE_TOKEN  table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" )  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  in header : cat_name = header  OPEN_SQUARE_TOKEN  header . index ( cat . split ( \"=\" )  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  )  CLOSE_SQUARE_TOKEN  conditions  OPEN_SQUARE_TOKEN  cat_name  CLOSE_SQUARE_TOKEN  . add ( cat . split ( \"=\" )  OPEN_SQUARE_TOKEN  1  CLOSE_SQUARE_TOKEN  ) if not cat_ids and not conditions : return  OPEN_CURLY_TOKEN  \"default\" : DataCategory ( set ( imap . keys ( ) ) ,  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  )  CLOSE_CURLY_TOKEN  if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join (  OPEN_SQUARE_TOKEN  row  OPEN_SQUARE_TOKEN  cid  CLOSE_SQUARE_TOKEN  for cid in cat_ids  CLOSE_SQUARE_TOKEN  ) if cat_name not in table : table  OPEN_SQUARE_TOKEN  cat_name  CLOSE_SQUARE_TOKEN  = DataCategory ( set ( ) ,  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ) table  OPEN_SQUARE_TOKEN  cat_name  CLOSE_SQUARE_TOKEN  . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all (  OPEN_SQUARE_TOKEN  row  OPEN_SQUARE_TOKEN  header . index ( c )  CLOSE_SQUARE_TOKEN  in conditions  OPEN_SQUARE_TOKEN  c  CLOSE_SQUARE_TOKEN  for c in conditions  CLOSE_SQUARE_TOKEN  ) : key = \"_\" . join (  OPEN_SQUARE_TOKEN  row  OPEN_SQUARE_TOKEN  idx  CLOSE_SQUARE_TOKEN  for idx in idx_to_test  CLOSE_SQUARE_TOKEN  ) try : assert key in table . keys ( ) except AssertionError : table  OPEN_SQUARE_TOKEN  key  CLOSE_SQUARE_TOKEN  = DataCategory ( set ( ) ,  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  ) table  OPEN_SQUARE_TOKEN  key  CLOSE_SQUARE_TOKEN  . sids . add ( sid ) try : assert len ( table )  GREATER_TOKEN  0 except AssertionError : return  OPEN_CURLY_TOKEN  \"default\" : DataCategory ( set ( imap . keys ( ) ) ,  OPEN_CURLY_TOKEN   CLOSE_CURLY_TOKEN  )  CLOSE_CURLY_TOKEN  else : return table', 'target': b'Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .'}\n",
            "{'input': b'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \" INDENT \" ) lines =  OPEN_SQUARE_TOKEN  line . strip ( ) for line in uF  CLOSE_SQUARE_TOKEN  unifrac =  OPEN_CURLY_TOKEN  \"pcd\" : OrderedDict ( ) , \"eigvals\" :  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN  , \"varexp\" :  OPEN_SQUARE_TOKEN   CLOSE_SQUARE_TOKEN   CLOSE_CURLY_TOKEN  if first  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first  OPEN_SQUARE_TOKEN  0  CLOSE_SQUARE_TOKEN  == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )', 'target': b'Parses the unifrac results file into a dictionary'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spun7-5XMBM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56c2ffb-1582-4f85-bf00-c657ea9759bb"
      },
      "source": [
        "t5.data.TaskRegistry.remove('python')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"python\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_python,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f438860c3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceK0-uXzl7EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49347fe4-7a57-4efe-b57c-bd3aac98a3e0"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"all_mix\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_mix\",\n",
        "    [\n",
        "     'go',\n",
        "     'ruby',\n",
        "     'js',\n",
        "     'php',\n",
        "     'java',\n",
        "     'python',\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7f43882c8650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSzYqw_l7EP"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "\n",
        "PRETRAINED_DIR = \"gs://cotext/cc/\"\n",
        "\n",
        "MODEL_DIR = \"gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkygCUDl7EQ"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW"
      },
      "source": [
        "tasks = [\n",
        "         ['codesearchnet', 'python'],\n",
        "         ['codesearchnet', 'java'],\n",
        "         ['codesearchnet', 'javascript'],\n",
        "         ['codesearchnet', 'go'],\n",
        "         ['codesearchnet', 'php'],\n",
        "         ['codesearchnet', 'ruby'],\n",
        "         ]\n",
        "output_dir = \"codesummarization_code_all_codesearchnet_v1\"\n",
        "test_file = 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6VR3fJS7y2i",
        "outputId": "8de6a945-091d-4049-f5f3-a866efd91d0b"
      },
      "source": [
        "for task in tasks:\n",
        "  lang = task[1]\n",
        "  !mkdir {lang}\n",
        "  !gsutil cp gs://cotext/data/{task[0]}/{lang}/{test_file}.tsv {lang}/\n",
        "  with open(f'{lang}/{test_file}.tsv', 'r') as file:\n",
        "    with open(f'{lang}/predict_input.tsv', 'w') as predict_input:\n",
        "      with open(f'{lang}/actual_output.tsv', 'w') as actual_output:\n",
        "        for line in file:\n",
        "          line = line.strip().split('\\t')\n",
        "          input = line[0].strip()\n",
        "          actual = line[1].strip()\n",
        "\n",
        "          predict_input.write(f'{lang}: {input}\\n')\n",
        "          actual_output.write(f'{actual}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/python/test.tsv...\n",
            "/ [1 files][  7.9 MiB/  7.9 MiB]                                                \n",
            "Operation completed over 1 objects/7.9 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/java/test.tsv...\n",
            "/ [1 files][  5.8 MiB/  5.8 MiB]                                                \n",
            "Operation completed over 1 objects/5.8 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/javascript/test.tsv...\n",
            "/ [1 files][  1.8 MiB/  1.8 MiB]                                                \n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/go/test.tsv...\n",
            "/ [1 files][  3.9 MiB/  3.9 MiB]                                                \n",
            "Operation completed over 1 objects/3.9 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/php/test.tsv...\n",
            "/ [1 files][  7.0 MiB/  7.0 MiB]                                                \n",
            "Operation completed over 1 objects/7.0 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/codesearchnet/ruby/test.tsv...\n",
            "/ [1 files][540.6 KiB/540.6 KiB]                                                \n",
            "Operation completed over 1 objects/540.6 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzDXwIlI16q",
        "outputId": "b82b648c-a9dc-4dcb-ac1c-bc22a978f562"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  lang = t[1]\n",
        "  input_file = f'{lang}/predict_input.tsv'\n",
        "  output_file = f'{lang}/predict_output.tsv'\n",
        "\n",
        "  predict_inputs_path = input_file\n",
        "  predict_outputs_path = output_file\n",
        "\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "  print(predict_inputs_path)\n",
        "  print(predict_outputs_path)\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        checkpoint_steps=-1,\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + lang)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python/predict_input.tsv\n",
            "python/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : python\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "java/predict_input.tsv\n",
            "java/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n",
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : java\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "javascript/predict_input.tsv\n",
            "javascript/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n",
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : javascript\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "go/predict_input.tsv\n",
            "go/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n",
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : go\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "php/predict_input.tsv\n",
            "php/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n",
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codesummarization_code_all_codesearchnet_v1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : php\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n",
            "ruby/predict_input.tsv\n",
            "ruby/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Could not extract mixture/task name from gin config.\n",
            "WARNING:absl:Using default vocabulary.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : ruby\n",
            "\n",
            "Predictions using checkpoint 1245000:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['codesearchnet', 'python'],\n",
        "         ['codesearchnet', 'java'],\n",
        "         ['codesearchnet', 'javascript'],\n",
        "         ['codesearchnet', 'go'],\n",
        "         ['codesearchnet', 'php'],\n",
        "         ['codesearchnet', 'ruby'],\n",
        "         ]\n",
        "output_dir = \"codesummarization_code_all_codesearchnet_v1\"\n",
        "test_file = 'test'\n",
        "checkpoint = '1245000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7odWGzG7p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e345d996-f77a-4727-b7d6-409823e9c118"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/evaluator.py\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/predictions.txt\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/reference.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-23 05:32:33--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/evaluator.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6916 (6.8K) [text/plain]\n",
            "Saving to: ‘evaluator.py’\n",
            "\n",
            "\revaluator.py          0%[                    ]       0  --.-KB/s               \revaluator.py        100%[===================>]   6.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 05:32:33 (70.0 MB/s) - ‘evaluator.py’ saved [6916/6916]\n",
            "\n",
            "--2021-04-23 05:32:33--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/predictions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196 [text/plain]\n",
            "Saving to: ‘predictions.txt’\n",
            "\n",
            "predictions.txt     100%[===================>]     196  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 05:32:34 (8.32 MB/s) - ‘predictions.txt’ saved [196/196]\n",
            "\n",
            "--2021-04-23 05:32:34--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Code-Text/code-to-text/evaluator/reference.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381 [text/plain]\n",
            "Saving to: ‘reference.txt’\n",
            "\n",
            "reference.txt       100%[===================>]     381  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-23 05:32:34 (19.0 MB/s) - ‘reference.txt’ saved [381/381]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfCRXczMMaeF",
        "outputId": "1d953775-d0d9-45ee-ee36-626124abd9ff"
      },
      "source": [
        "!python evaluator.py reference.txt < predictions.txt\n",
        "!mkdir output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total: 5\n",
            "9.554726113590661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gURjp726NqO1",
        "outputId": "28d7845d-e9b6-4c3f-e183-fb4ff6e4cbdd"
      },
      "source": [
        "for task in tasks:\n",
        "  lang = task[1]\n",
        "  with open(f'{lang}/predict_output.tsv-{checkpoint}') as predict_output:\n",
        "    with open(f'{lang}/actual_output.tsv') as actual_output:\n",
        "      with open(f'output/{lang}_reference.txt', 'w') as reference:\n",
        "        with open(f'output/{lang}_predictions.txt', 'w') as predictions:\n",
        "          for idx, (line1, line2) in enumerate(zip(predict_output, actual_output)):\n",
        "            line1 = line1.replace('⁇', '')\n",
        "            reference.write(f'{idx}\\t{line2}')\n",
        "            predictions.write(f'{idx}\\t{line1}')\n",
        "          print(f'language: {lang}')\n",
        "          !python evaluator.py output/{lang}_reference.txt < output/{lang}_predictions.txt\n",
        "          print('\\n')\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language: python\n",
            "Total: 14884\n",
            "19.35622797473649\n",
            "\n",
            "\n",
            "language: java\n",
            "Total: 10834\n",
            "18.749421575274635\n",
            "\n",
            "\n",
            "language: javascript\n",
            "Total: 3180\n",
            "14.75286010413368\n",
            "\n",
            "\n",
            "language: go\n",
            "Total: 8076\n",
            "18.952009265859775\n",
            "\n",
            "\n",
            "language: php\n",
            "Total: 13951\n",
            "22.968755076406474\n",
            "\n",
            "\n",
            "language: ruby\n",
            "Total: 1198\n",
            "13.232945480742675\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}