{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "CodeGenerationUni.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq7fFLtrO_r1"
      },
      "source": [
        "#all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF9U1ph4ISIN",
        "outputId": "c00d352b-599d-4133-ea27-df511da07e12"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K     |████████████████████████████████| 235kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 57.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 55.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 45.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 47.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.2MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-V5LV3DI7IR",
        "outputId": "95f5bc6f-54e7-4259-875c-64011a2c70bf"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up GCS access...\n",
            "Running on TPU: grpc://10.42.248.130:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPqdkqykJJWi",
        "outputId": "441c801c-7089-4917-d18c-350d6fc907a6"
      },
      "source": [
        "print(t5.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6jwz87ZHjG5"
      },
      "source": [
        "### Register concode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqMaPzwkETAK",
        "outputId": "46d421aa-0de3-4338-e0b5-d3259519caa1"
      },
      "source": [
        "def dumping_dataset_java(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/codegeneration/code_generation_train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"java: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset_java(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'check if details are parsed . concode_field_sep Container parent concode_elem_sep boolean isParsed concode_elem_sep long offset concode_elem_sep long contentStartPosition concode_elem_sep ByteBuffer deadBytes concode_elem_sep boolean isRead concode_elem_sep long memMapSize concode_elem_sep Logger LOG concode_elem_sep byte[] userType concode_elem_sep String type concode_elem_sep ByteBuffer content concode_elem_sep FileChannel fileChannel concode_field_sep Container getParent concode_elem_sep byte[] getUserType concode_elem_sep void readContent concode_elem_sep long getOffset concode_elem_sep long getContentSize concode_elem_sep void getContent concode_elem_sep void setDeadBytes concode_elem_sep void parse concode_elem_sep void getHeader concode_elem_sep long getSize concode_elem_sep void parseDetails concode_elem_sep String getType concode_elem_sep void _parseDetails concode_elem_sep String getPath concode_elem_sep boolean verify concode_elem_sep void setParent concode_elem_sep void getBox concode_elem_sep boolean isSmallBox', 'target': b'boolean function ( ) OPEN_CURLY_TOKEN return isParsed ; CLOSE_CURLY_TOKEN'}\n",
            "{'input': b'answer the library file defining the library containing the compilation unit to be indexed or null if the library is not on disk concode_field_sep IndexStore indexStore concode_elem_sep IndexPerformanceRecorder performanceRecorder concode_elem_sep DartUnit unit concode_elem_sep CompilationUnit compilationUnit concode_elem_sep Resource resource concode_elem_sep File libraryFile concode_field_sep boolean removeWhenResourceRemoved concode_elem_sep CompilationUnit getCompilationUnit concode_elem_sep boolean isQuery concode_elem_sep String toString concode_elem_sep void performOperation', 'target': b'File function ( ) OPEN_CURLY_TOKEN return libraryFile ; CLOSE_CURLY_TOKEN'}\n",
            "{'input': b'this method deletes index files of the @linkplain indexcommit for the specified generation number . concode_field_sep Logger log concode_field_sep void deleteNonSnapshotIndexFiles concode_elem_sep Map<String,Integer> buildRefCounts', 'target': b'void function ( Directory arg0 , Collection SMALLER_TOKEN SnapshotMetaData GREATER_TOKEN arg1 , long arg2 ) OPEN_CURLY_TOKEN List SMALLER_TOKEN IndexCommit GREATER_TOKEN loc0 = DirectoryReader . listCommits ( arg0 ) ; Map SMALLER_TOKEN String , Integer GREATER_TOKEN loc1 = buildRefCounts ( arg1 , loc0 ) ; for ( IndexCommit loc2 : loc0 ) OPEN_CURLY_TOKEN if ( loc2 . getGeneration ( ) == arg2 ) OPEN_CURLY_TOKEN deleteIndexFiles ( arg0 , loc1 , loc2 ) ; break ; CLOSE_CURLY_TOKEN CLOSE_CURLY_TOKEN CLOSE_CURLY_TOKEN'}\n",
            "{'input': b\"do n't use this . no , really , do n't use this . you already have an authenticationtoken with org.apache.accumulo.core.client.mapreduce.lib.impl.configuratorbase #getauthenticationtoken class , configuration . you do n't need to construct it yourself . gets the password from the configuration . warning : the password is stored in the configuration and shared with all mapreduce tasks ; it is base64 encoded to provide a charset safe conversion to a string , and is not intended to be secure . concode_field_sep PlaceHolder placeHolder concode_field_sep String getPrincipal concode_elem_sep void setLogLevel concode_elem_sep Level getLogLevel concode_elem_sep Boolean isConnectorInfoSet concode_elem_sep String getTokenClass concode_elem_sep void setZooKeeperInstance concode_elem_sep void setMockInstance concode_elem_sep Instance getInstance concode_elem_sep String enumToConfKey concode_elem_sep void setConnectorInfo\", 'target': b'byte OPEN_SQUARE_TOKEN CLOSE_SQUARE_TOKEN function ( Class SMALLER_TOKEN ? GREATER_TOKEN arg0 , Configuration arg1 ) OPEN_CURLY_TOKEN return AuthenticationTokenSerializer . serialize ( org . apache . accumulo . core . client . mapreduce . lib . impl . ConfiguratorBase . getAuthenticationToken ( arg0 , arg1 ) ) ; CLOSE_CURLY_TOKEN'}\n",
            "{'input': b'force the eventbus from ambarieventpublisher to be serialand synchronous . concode_field_sep PlaceHolder placeHolder concode_field_sep void registerAlertListeners concode_elem_sep EventBus synchronizeAlertEventPublisher concode_elem_sep void replaceEventBus concode_elem_sep void registerAmbariListeners', 'target': b'void function ( Binder arg0 ) OPEN_CURLY_TOKEN EventBus loc0 = new EventBus ( ) ; AmbariEventPublisher loc1 = new AmbariEventPublisher ( ) ; replaceEventBus ( AmbariEventPublisher . class , loc1 , loc0 ) ; arg0 . bind ( AmbariEventPublisher . class ) . toInstance ( loc1 ) ; CLOSE_CURLY_TOKEN'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYrp9wFFEuI7",
        "outputId": "d6b5f16f-63b3-41b8-c993-2542266778b6"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset_java,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f7a757dbdd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceK0-uXzl7EO",
        "outputId": "7e016bab-87f1-4c54-bf16-f5ad60d2ad5c"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"all_mix\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_mix\",\n",
        "    [\n",
        "     'java',\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7f7a75835710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmhW82e9MzuF"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "PRETRAINED_DIR = \"gs://cotext/cc\"\n",
        "\n",
        "MODEL_DIR = \"gs://t5_training/models/code/codegeneration_uni_v1/\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 256},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBkygCUDl7EQ",
        "outputId": "173ff3f0-56df-4a01-aa1e-ab7f2fcae808"
      },
      "source": [
        "FINETUNE_STEPS = 57000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/code_uni_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/code_uni_v1/base/operative_config.gin\n",
            "INFO:root:Skipping import of unknown module `t5.data.sentencepiece_vocabulary` (skip_unknown=True).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://t5_training/models/code/codegeneration_uni_v1/base', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.42.248.130:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.42.248.130:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.42.248.130:8470', '_evaluation_master': 'grpc://10.42.248.130:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f7a757d4d10>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW"
      },
      "source": [
        "tasks = [['codegeneration', 'java']]\n",
        "output_dir = \"codegeneration_uni_v1\"\n",
        "test_file = 'test'\n",
        "test_folder = 'valid3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6VR3fJS7y2i",
        "outputId": "deb590da-2a70-4665-d009-31b4f5cc33ba"
      },
      "source": [
        "!mkdir {test_folder}\n",
        "!gsutil cp gs://cotext/data/codegeneration/code_generation_valid.tsv {test_folder}/ \n",
        "with open(f'{test_folder}/code_generation_valid.tsv', 'r') as file:\n",
        "  with open(f'{test_folder}/predict_input.tsv', 'w') as predict_input:\n",
        "    for line in file:\n",
        "      line = line.strip().split('\\t')\n",
        "      input = line[0].strip()\n",
        "      predict_input.write(f'java: {input}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://t5_training/t5-data/code_data/codegeneration/code_generation_valid.tsv...\n",
            "/ [0 files][    0.0 B/  1.8 MiB]                                                \r/ [1 files][  1.8 MiB/  1.8 MiB]                                                \r\n",
            "Operation completed over 1 objects/1.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzDXwIlI16q",
        "outputId": "da7b346e-125b-438b-e914-a47b3dcde93c"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  lang = t[1]\n",
        "  input_file = f'{test_folder}/predict_input.tsv'\n",
        "  output_file = f'{test_folder}/predict_output.tsv'\n",
        "  predict_inputs_path = input_file\n",
        "  predict_outputs_path = output_file\n",
        "\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "  print(predict_inputs_path)\n",
        "  print(predict_outputs_path)\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        checkpoint_steps=-1,\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + lang)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid3/predict_input.tsv\n",
            "valid3/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/codegeneration_uni_v1/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/codegeneration_uni_v1/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : java\n",
            "\n",
            "Predictions using checkpoint 1260000:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI6LdBtg_vwl"
      },
      "source": [
        "checkpoint = '1255000'\n",
        "with open(f'test1/predict_output.tsv-{checkpoint}') as file:\n",
        "  with open('predictions_uni_55k.txt', 'w') as out:\n",
        "    for line in file:\n",
        "      pred = line.strip().replace('SMALLER_TOKEN', '<').replace('GREATER_TOKEN', '>').replace('OPEN_SQUARE_TOKEN', '[').replace('CLOSE_SQUARE_TOKEN', ']').replace('OPEN_CURLY_TOKEN', '{').replace('CLOSE_CURLY_TOKEN', '}').replace('EXPONENTIAL_TOKEN', '^').replace('SHARP_TOKEN', '#').replace('DOLLAR_TOKEN', '$').replace('UNK_TOKEN', '`')\n",
        "      out.write(pred + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['codegeneration', 'java']\n",
        "         ]\n",
        "output_dir = \"codegeneration_uni_v1\"\n",
        "test_file = 'test'\n",
        "checkpoint = '1260000'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av7odWGzG7p6",
        "outputId": "5a8828f0-5e71-429a-87ef-a739e49b8380"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/bleu.py\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/evaluator.py\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/answers.json\n",
        "!wget https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/predictions.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-13 07:20:27--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4877 (4.8K) [text/plain]\n",
            "Saving to: ‘bleu.py’\n",
            "\n",
            "\rbleu.py               0%[                    ]       0  --.-KB/s               \rbleu.py             100%[===================>]   4.76K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-13 07:20:27 (59.1 MB/s) - ‘bleu.py’ saved [4877/4877]\n",
            "\n",
            "--2021-04-13 07:20:27--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/evaluator.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1390 (1.4K) [text/plain]\n",
            "Saving to: ‘evaluator.py’\n",
            "\n",
            "evaluator.py        100%[===================>]   1.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-13 07:20:27 (12.9 MB/s) - ‘evaluator.py’ saved [1390/1390]\n",
            "\n",
            "--2021-04-13 07:20:27--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/answers.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93083 (91K) [text/plain]\n",
            "Saving to: ‘answers.json’\n",
            "\n",
            "answers.json        100%[===================>]  90.90K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-04-13 07:20:28 (6.45 MB/s) - ‘answers.json’ saved [93083/93083]\n",
            "\n",
            "--2021-04-13 07:20:28--  https://raw.githubusercontent.com/microsoft/CodeXGLUE/main/Text-Code/text-to-code/evaluator/predictions.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6663 (6.5K) [text/plain]\n",
            "Saving to: ‘predictions.txt’\n",
            "\n",
            "predictions.txt     100%[===================>]   6.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-13 07:20:28 (58.1 MB/s) - ‘predictions.txt’ saved [6663/6663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfCRXczMMaeF",
        "outputId": "798dd648-debb-4f16-ae03-143c253f6af8"
      },
      "source": [
        "!python evaluator.py -a=answers.json -p=predictions.txt\n",
        "# !mkdir output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:BLEU: 16.68, EM: 17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZREqOtIYPt5",
        "outputId": "6b30e9d4-ee82-46db-d27e-45dca83e2bb2"
      },
      "source": [
        "import json\n",
        "for task in tasks:\n",
        "  lang = task[1]\n",
        "  with open(f'{test_folder}/code_generation_valid.tsv', 'r') as test_file:\n",
        "    with open(f'{test_folder}/answers.json', 'w') as out_test:\n",
        "      for line in test_file:\n",
        "        line = line.strip().split('\\t')\n",
        "        nl = line[0].strip()\n",
        "        code = line[1].strip().replace('SMALLER_TOKEN', '<').replace('GREATER_TOKEN', '>').replace('OPEN_SQUARE_TOKEN', '[').replace('CLOSE_SQUARE_TOKEN', ']').replace('OPEN_CURLY_TOKEN', '{').replace('CLOSE_CURLY_TOKEN', '}').replace('EXPONENTIAL_TOKEN', '^').replace('SHARP_TOKEN', '#').replace('DOLLAR_TOKEN', '$').replace('UNK_TOKEN', '`')\n",
        "        li = {\"code\": code, \"nl\": nl}\n",
        "        out_test.write(json.dumps(li))\n",
        "        out_test.write('\\n')\n",
        "  with open(f'{test_folder}/predict_output.tsv-{checkpoint}') as predict_output:\n",
        "    with open(f'{test_folder}/predictions.txt', 'w') as predict_file:\n",
        "      for line in predict_output:\n",
        "        line = line.strip().replace('SMALLER_TOKEN', '<').replace('GREATER_TOKEN', '>').replace('OPEN_SQUARE_TOKEN', '[').replace('CLOSE_SQUARE_TOKEN', ']').replace('OPEN_CURLY_TOKEN', '{').replace('CLOSE_CURLY_TOKEN', '}').replace('EXPONENTIAL_TOKEN', '^').replace('SHARP_TOKEN', '#').replace('DOLLAR_TOKEN', '$').replace('UNK_TOKEN', '`')\n",
        "        predict_file.write(line)\n",
        "        predict_file.write('\\n')\n",
        "  print(f'language: {lang}')\n",
        "  !python evaluator.py -a={test_folder}/answers.json -p={test_folder}/predictions.txt\n",
        "  print('\\n')\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "language: java\n",
            "INFO:__main__:BLEU: 33.43, EM: 17.25\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}