{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bug2fix_1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPd3O1Xl7CA"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Ar53sBl7CB"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDVvzNA7l7CB",
        "outputId": "a2803d5b-fae5-4cb2-feb3-c8c6985c8b89"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\u001b[K     |████████████████████████████████| 153kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 40.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 40.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 36.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 44.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 47.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1CKuTdVl7CE",
        "outputId": "081985d3-2727-418c-f1ea-6ed1eaff98d2"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up GCS access...\n",
            "Running on TPU: grpc://10.78.126.218:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3EkFkJ21G6p"
      },
      "source": [
        "# print(mesh_tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A969X_jAr2qE",
        "outputId": "7e102649-513e-4a92-d746-62c6cdd30d4b"
      },
      "source": [
        "print(t5.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPI1TmAeh0_"
      },
      "source": [
        "# import gin\n",
        "# import subprocess\n",
        "# gin.parse_config_file(\n",
        "#         'gs://t5-data/pretrained_models/base/operative_config.gin'\n",
        "#     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtIvOasl7CI"
      },
      "source": [
        "## Register Tasks Medium\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnIhn9K_MBM-",
        "outputId": "9963e5fe-6ca4-497f-f87c-d2ed5aa2688b"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/bug2fix/medium/train.tsv',\n",
        "            'gs://cotext/data/bug2fix/medium/valid.tsv',\n",
        "\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"medium: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public static TYPE_1 init ( java.lang.String name , java.util.Date date )  OPEN_CURLY_TOKEN  TYPE_1 VAR_1 = new TYPE_1 ( ) ; VAR_1 . METHOD_1 ( name ) ; java.util.Calendar VAR_2 = java.util.Calendar.getInstance ( ) ; VAR_2 . METHOD_2 ( date ) ; VAR_1 . METHOD_3 ( VAR_2 ) ; return VAR_1 ;  CLOSE_CURLY_TOKEN ', 'target': b'public static TYPE_1 init ( java.lang.String name , java.util.Date date )  OPEN_CURLY_TOKEN  TYPE_1 VAR_1 = new TYPE_1 ( ) ; VAR_1 . METHOD_1 ( name ) ; java.util.Calendar VAR_2 = null ; if ( date != null )  OPEN_CURLY_TOKEN  VAR_2 = java.util.Calendar.getInstance ( ) ; VAR_2 . METHOD_2 ( date ) ;  CLOSE_CURLY_TOKEN  VAR_1 . METHOD_3 ( VAR_2 ) ; return VAR_1 ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public TYPE_1 METHOD_1 ( java.lang.String name )  OPEN_CURLY_TOKEN  if ( name . equals ( STRING_1 ) ) return new TYPE_2 ( STRING_2 , true ) ; if ( name . equals ( STRING_3 ) ) return new TYPE_3 ( STRING_4 , true ) ; if ( name . equals ( STRING_5 ) ) return new TYPE_4 ( ) ; return super . METHOD_1 ( name ) ;  CLOSE_CURLY_TOKEN ', 'target': b'public TYPE_1 METHOD_1 ( java.lang.String name )  OPEN_CURLY_TOKEN  if ( name . equals ( STRING_3 ) ) return new TYPE_3 ( STRING_4 , true ) ; if ( name . equals ( STRING_5 ) ) return new TYPE_4 ( ) ; return super . METHOD_1 ( name ) ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'private boolean METHOD_1 ( TYPE_1 VAR_1 )  OPEN_CURLY_TOKEN  boolean VAR_2 = false ; VAR_2 = VAR_2 || ( ( VAR_3 . compareTo ( VAR_1 . METHOD_2 ( ) ) )  SMALLER_TOKEN  0 ) ; VAR_2 = VAR_2 || ( ! ( VAR_1 . METHOD_3 ( ) . METHOD_4 ( ) . equals ( VAR_4 ) ) ) ; return VAR_2 ;  CLOSE_CURLY_TOKEN ', 'target': b'private boolean METHOD_1 ( TYPE_1 VAR_1 )  OPEN_CURLY_TOKEN  boolean VAR_2 = ( VAR_3 . compareTo ( VAR_1 . METHOD_2 ( ) ) )  SMALLER_TOKEN  0 ; VAR_2 = VAR_2 || ( ! ( VAR_1 . METHOD_3 ( ) . METHOD_4 ( ) . equals ( VAR_4 ) ) ) ; return VAR_2 ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public void METHOD_1 ( TYPE_1 VAR_1 , boolean VAR_2 )  OPEN_CURLY_TOKEN  if ( VAR_2 )  OPEN_CURLY_TOKEN  VAR_3 . METHOD_2 ( 1 , CHAR_1 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  VAR_3 . METHOD_2 ( 1 , CHAR_2 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ;  CLOSE_CURLY_TOKEN  TYPE_2 VAR_5 = TYPE_2 . METHOD_4 ( getActivity ( ) , VAR_4 . METHOD_5 ( ) , VAR_6 ) ; VAR_5 . show ( ) ;  CLOSE_CURLY_TOKEN ', 'target': b'public void METHOD_1 ( TYPE_1 VAR_1 , boolean VAR_2 )  OPEN_CURLY_TOKEN  if ( VAR_2 )  OPEN_CURLY_TOKEN  VAR_3 . METHOD_2 ( 1 , CHAR_1 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ;  CLOSE_CURLY_TOKEN  else  OPEN_CURLY_TOKEN  VAR_3 . METHOD_2 ( 1 , CHAR_2 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ;  CLOSE_CURLY_TOKEN   CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public boolean METHOD_1 ( )  OPEN_CURLY_TOKEN  if ( ( VAR_1 ) == ( ( VAR_2 . METHOD_2 ( VAR_1 ) ) - 1 ) )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  if ( ( METHOD_3 ( ) . getValue ( ) )  SMALLER_TOKEN = ( METHOD_4 ( ( ( VAR_1 ) + 1 ) , VAR_3 ) . getValue ( ) ) )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  ( VAR_1 ) ++ ; return true ;  CLOSE_CURLY_TOKEN ', 'target': b'public boolean METHOD_1 ( )  OPEN_CURLY_TOKEN  if ( ( VAR_1 )  GREATER_TOKEN = ( ( VAR_2 . METHOD_2 ( VAR_3 ) ) - 1 ) )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  if ( ( METHOD_3 ( ) . getValue ( ) )  SMALLER_TOKEN = ( METHOD_4 ( ( ( VAR_1 ) + 1 ) , VAR_3 ) . getValue ( ) ) )  OPEN_CURLY_TOKEN  return false ;  CLOSE_CURLY_TOKEN  ( VAR_1 ) ++ ; return true ;  CLOSE_CURLY_TOKEN '}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spun7-5XMBM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b87456-f1c1-4910-9396-acb57066904c"
      },
      "source": [
        "t5.data.TaskRegistry.remove('medium')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"medium\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    # metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "    #            t5.evaluation.metrics.sequence_accuracy, \n",
        "    #             ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f94ee4ec710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGQirLNeX0dr"
      },
      "source": [
        "## Register Tasks small\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q63OQCTnX0dt",
        "outputId": "9a352c66-c38b-4f95-e2cf-28a9ccd410e2"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://cotext/data/bug2fix/small/train.tsv',\n",
        "            'gs://cotext/data/bug2fix/small/valid.tsv',\n",
        "\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"small: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw validation examples...\n",
            "{'input': b'public java.lang.String METHOD_1 ( )  OPEN_CURLY_TOKEN  return new TYPE_1 ( STRING_1 ) . format ( VAR_1  OPEN_SQUARE_TOKEN  ( ( VAR_1 . length ) - 1 )  CLOSE_SQUARE_TOKEN  . getTime ( ) ) ;  CLOSE_CURLY_TOKEN ', 'target': b'public java.lang.String METHOD_1 ( )  OPEN_CURLY_TOKEN  return new TYPE_1 ( STRING_1 ) . format ( VAR_1  OPEN_SQUARE_TOKEN  ( ( type ) - 1 )  CLOSE_SQUARE_TOKEN  . getTime ( ) ) ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public boolean METHOD_1 ( java.lang.String name )  OPEN_CURLY_TOKEN  TYPE_1 VAR_1 = TYPE_1 . METHOD_2 ( VAR_2 ) ; return ( ! ( METHOD_3 ( name ) ) ) && ( VAR_1 . contains ( name ) ) ;  CLOSE_CURLY_TOKEN ', 'target': b'public boolean METHOD_1 ( java.lang.String name )  OPEN_CURLY_TOKEN  return ( ! ( METHOD_3 ( name ) ) ) && ( TYPE_1 . METHOD_2 ( VAR_2 ) . contains ( name ) ) ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public char METHOD_1 ( java.lang.String VAR_1 , java.lang.String name )  OPEN_CURLY_TOKEN  return null ;  CLOSE_CURLY_TOKEN ', 'target': b'public char METHOD_1 ( java.lang.String VAR_1 , java.lang.String name )  OPEN_CURLY_TOKEN  return 0 ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'private void METHOD_1 ( )  OPEN_CURLY_TOKEN  VAR_1 . METHOD_2 ( STRING_1 ) ; VAR_2 = false ; ( VAR_3 ) ++ ; if ( ( VAR_3 ) == ( VAR_4 . size ( ) ) )  OPEN_CURLY_TOKEN  METHOD_3 ( ) ;  CLOSE_CURLY_TOKEN  METHOD_4 ( ) ;  CLOSE_CURLY_TOKEN ', 'target': b'private void METHOD_1 ( )  OPEN_CURLY_TOKEN  VAR_1 . METHOD_2 ( STRING_1 ) ; VAR_2 = false ; ( VAR_3 ) ++ ; if ( ( VAR_3 ) == ( VAR_4 . size ( ) ) )  OPEN_CURLY_TOKEN  METHOD_3 ( ) ; return ;  CLOSE_CURLY_TOKEN  METHOD_4 ( ) ;  CLOSE_CURLY_TOKEN '}\n",
            "{'input': b'public TYPE_1 METHOD_1 ( )  OPEN_CURLY_TOKEN  java.lang.System.out.println ( STRING_1 ) ; return this . VAR_1 . METHOD_1 ( ) ;  CLOSE_CURLY_TOKEN ', 'target': b'public TYPE_1 METHOD_1 ( )  OPEN_CURLY_TOKEN  return this . VAR_1 . METHOD_1 ( ) ;  CLOSE_CURLY_TOKEN '}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QKOr2cPX0du",
        "outputId": "b53782ba-de1c-4104-833e-7c2e99048539"
      },
      "source": [
        "t5.data.TaskRegistry.remove('small')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"small\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f94ee4ec910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceK0-uXzl7EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f050fe06-9213-419a-b12e-9e40eff287dd"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"all_mix\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_mix\",\n",
        "    [\n",
        "     'medium',\n",
        "     'small'\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7f94bb86e850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwvZLQy4wv0I"
      },
      "source": [
        "# !gsutil -m rm -r {MODEL_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSzYqw_l7EP"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "BASE_PRETRAINED_DIR = \"gs://cotext/cc/\"\n",
        "\n",
        "PRETRAINED_DIR = BASE_PRETRAINED_DIR\n",
        "\n",
        "MODEL_DIR = \"gs://t5_training/models/code/bug2fix_base_v2/\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkygCUDl7EQ"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"all_mix\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c67cb3-713c-4a45-9885-1d5a3ca46336"
      },
      "source": [
        "tasks = [\n",
        "         ['bug2fix', 'small'],\n",
        "         ['bug2fix', 'medium']\n",
        "         ]\n",
        "output_dir = \"bug2fix_base_v2\"\n",
        "test_file = 'test'\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6VR3fJS7y2i",
        "outputId": "1c96a0b0-0daf-4674-f71c-a46e5d6e7f6b"
      },
      "source": [
        "for task in tasks:\n",
        "  !mkdir {task[1]}\n",
        "  !gsutil cp gs://cotext/data/{task[0]}/{task[1]}/{test_file}.tsv {task[1]}/\n",
        "  with open(f'{task[1]}/{test_file}.tsv', 'r') as file:\n",
        "    with open(f'{task[1]}/predict_input.tsv', 'w') as predict_input:\n",
        "      with open(f'{task[1]}/actual_output.tsv', 'w') as actual_output:\n",
        "        for line in file:\n",
        "          line = line.strip().split('\\t')\n",
        "          input = line[0].strip()\n",
        "          \n",
        "          actual = line[1].strip()\n",
        "          actual = ' '.join(actual.split())\n",
        "          actual = actual.strip() \\\n",
        "                .replace(' SMALLER_TOKEN ', ' < ') \\\n",
        "                .replace(' GREATER_TOKEN ', ' > ')\\\n",
        "                .replace(' OPEN_SQUARE_TOKEN ', ' [ ')\\\n",
        "                .replace(' CLOSE_SQUARE_TOKEN ', ' ] ')\\\n",
        "                .replace(' OPEN_CURLY_TOKEN ', ' { ')\\\n",
        "                .replace(' CLOSE_CURLY_TOKEN', ' } ')\\\n",
        "                .replace(' CLOSE_CURLY_TOKEN ', ' } ')\\\n",
        "                .replace(' EXPONENTIAL_TOKEN ', ' ^ ')\\\n",
        "                .replace(' SHARP_TOKEN ', ' # ')\\\n",
        "                .replace(' DOLLAR_TOKEN ', ' $ ')\\\n",
        "                .replace(' UNK_TOKEN ', ' ` ') \\\n",
        "                .replace(' NEW_LINE ', ' \\\\n ') \\\n",
        "                .replace(' INDENT ', ' \\\\t ')\n",
        "\n",
        "          predict_input.write(f'{task[1]}: {input}\\n')\n",
        "          actual_output.write(f'{actual}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘small’: File exists\n",
            "Copying gs://t5_training/t5-data/code_data/bug2fix/small/test.tsv...\n",
            "/ [1 files][  2.1 MiB/  2.1 MiB]                                                \n",
            "Operation completed over 1 objects/2.1 MiB.                                      \n",
            "Copying gs://t5_training/t5-data/code_data/bug2fix/medium/test.tsv...\n",
            "/ [1 files][  5.2 MiB/  5.2 MiB]                                                \n",
            "Operation completed over 1 objects/5.2 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAzDXwIlI16q",
        "outputId": "185a994b-0657-4cd5-f70d-fdc63ff6a052"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "for task in tasks:\n",
        "  dir = task[0]\n",
        "  input_file = f'{task[1]}/predict_input.tsv'\n",
        "  output_file = f'{task[1]}/predict_output.tsv'\n",
        "\n",
        "  predict_inputs_path = input_file\n",
        "  predict_outputs_path = output_file\n",
        "\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "  print(predict_inputs_path)\n",
        "  print(predict_outputs_path)\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        checkpoint_steps=-1,\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + dir)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small/predict_input.tsv\n",
            "small/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/bug2fix_base_v2/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/bug2fix_base_v2/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : bug2fix\n",
            "\n",
            "Predictions using checkpoint 1044900:\n",
            "\n",
            "medium/predict_input.tsv\n",
            "medium/predict_output.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://t5_training/models/code/bug2fix_base_v2/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://t5_training/models/code/bug2fix_base_v2/base/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted task : bug2fix\n",
            "\n",
            "Predictions using checkpoint 1044900:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7odWGzG7p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e710403e-8dc5-43ce-91ac-f583307dc508"
      },
      "source": [
        "!git clone https://github.com/microsoft/CodeXGLUE.git\n",
        "%cd /content/CodeXGLUE/Code-Code/code-refinement"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CodeXGLUE'...\n",
            "remote: Enumerating objects: 2604, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (218/218), done.\u001b[K\n",
            "remote: Total 2604 (delta 160), reused 67 (delta 41), pack-reused 2343\u001b[K\n",
            "Receiving objects: 100% (2604/2604), 200.43 MiB | 21.63 MiB/s, done.\n",
            "Resolving deltas: 100% (1230/1230), done.\n",
            "Checking out files: 100% (385/385), done.\n",
            "/content/CodeXGLUE/Code-Code/code-refinement\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['bug2fix', 'small'],\n",
        "         ['bug2fix', 'medium']\n",
        "         ]\n",
        "# output_dir = \"defect_detection_code_all_codesearchnet_v1\"\n",
        "test_file = 'test'\n",
        "checkpoint = '1044900'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EMm0YAoMLS6",
        "outputId": "c2614700-ce78-4a56-e004-a74eb45a578d"
      },
      "source": [
        "for task in tasks:\n",
        "  print(task[0], task[1])\n",
        "  \n",
        "  with open(f'/content/{task[1]}/predict_output.tsv-{checkpoint}') as file:\n",
        "    with open(f'/content/{task[1]}/predict_output.tsv', 'w') as out_file:\n",
        "      for line in file:\n",
        "        line = line.strip() \\\n",
        "                .replace('SMALLER_TOKEN', '<') \\\n",
        "                .replace('GREATER_TOKEN', '>')\\\n",
        "                .replace('OPEN_SQUARE_TOKEN', '[')\\\n",
        "                .replace('CLOSE_SQUARE_TOKEN', ']')\\\n",
        "                .replace('OPEN_CURLY_TOKEN', '{')\\\n",
        "                .replace('CLOSE_CURLY_TOKEN', '}')\\\n",
        "                .replace('EXPONENTIAL_TOKEN', '^')\\\n",
        "                .replace('SHARP_TOKEN', '#')\\\n",
        "                .replace('DOLLAR_TOKEN', '$')\\\n",
        "                .replace('UNK_TOKEN', '`') \\\n",
        "                .replace('NEW_LINE', '\\\\n') \\\n",
        "                .replace('INDENT', '\\\\t')\n",
        "        out_file.write(f'{line}\\n')\n",
        "  \n",
        "  !python evaluator/evaluator.py -ref /content/{task[1]}/actual_output.tsv -pre /content/{task[1]}/predict_output.tsv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bug2fix small\n",
            "BLEU: 77.43 ; Acc: 16.04\n",
            "bug2fix medium\n",
            "BLEU: 88.28 ; Acc: 4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwujo5nonxZF",
        "outputId": "11d11143-30a9-4591-fba1-bcd8f1912a43"
      },
      "source": [
        "!pip install tree_sitter==0.2.0\n",
        "%cd /content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU\n",
        "\n",
        "for task in tasks:\n",
        "  print(task[0], task[1])\n",
        "  !python calc_code_bleu.py --refs /content/{task[1]}/actual_output.tsv --hyp /content/{task[1]}/predict_output.tsv --lang java"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tree_sitter==0.2.0 in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "/content/CodeXGLUE/Code-Code/code-to-code-trans/evaluator/CodeBLEU\n",
            "bug2fix small\n",
            "ngram match: 0.7742618221241298, weighted ngram match: 0.7872707245755032, syntax_match: 0.7397810520897484, dataflow_match: 0.7869320499225059\n",
            "CodeBLEU score:  0.7720614121779718\n",
            "bug2fix medium\n",
            "ngram match: 0.8827828757167933, weighted ngram match: 0.8833559860946272, syntax_match: 0.8254292584355915, dataflow_match: 0.8331314870234509\n",
            "CodeBLEU score:  0.8561749018176157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXufzHhn9gO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}